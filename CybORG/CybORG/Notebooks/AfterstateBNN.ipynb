{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDN References:\n",
    "\n",
    "[useful pytorch reference](https://github.com/tonyduan/mixture-density-network)\n",
    "\n",
    "[keras version](https://github.com/cpmpercussion/keras-mdn-layer)\n",
    "\n",
    "[another keras version](https://github.com/omimo/Keras-MDN/blob/master/kmdn/mdn.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MDN\n",
    "from MDN import sample_from_output\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "from ProcessTrueStateActionData import read_df_in_chunks\n",
    "\n",
    "from true_state_viewer import TrueStateTreeGraphViz, display_tree_red_preds\n",
    "\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data \n",
    "(produce tf train+test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_test_split = 0.98\n",
    "\n",
    "STATE_SIZE = 42 # (mdn output_dimension)\n",
    "NUMBER_MIXTURES = 10\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "DATA_CAP = 3_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3960\n"
     ]
    }
   ],
   "source": [
    "data_path = 'logs/APPO/TrueStates_200_1000_Meander_small/data'\n",
    "afterstates = np.load(data_path + '/afterstates.npy')\n",
    "next_states = np.load(data_path + '/next_states.npy')\n",
    "afterstates = np.array(afterstates, dtype=np.float32)\n",
    "next_states = np.array(next_states, dtype=np.float32)\n",
    "divide = int(train_test_split * afterstates.shape[0])\n",
    "afterstates_train = afterstates[:divide,:]\n",
    "next_states_train = next_states[:divide,:]\n",
    "afterstates_test = afterstates[divide:,:]\n",
    "next_states_test = next_states[divide:,:]\n",
    "print(afterstates.shape[0] - divide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices((afterstates_train, next_states_train)).shuffle(afterstates.shape[0]).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((afterstates_test, next_states_test)).shuffle(afterstates.shape[0]).batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an MDN based model with pretrained encoder/decoder layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 42)]              0         \n",
      "                                                                 \n",
      " dense_variational_31 (Dense  (None, 8)                59684     \n",
      " Variational)                                                    \n",
      "                                                                 \n",
      " dense_variational_32 (Dense  (None, 8)                2700      \n",
      " Variational)                                                    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 84)                756       \n",
      "                                                                 \n",
      " independent_normal_18 (Inde  ((None, 42),             0         \n",
      " pendentNormal)               (None, 42))                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,140\n",
      "Trainable params: 63,140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/250\n",
      "1365/1365 [==============================] - 12s 6ms/step - loss: 26.1518 - root_mean_squared_error: 0.7592 - val_loss: 13.7593 - val_root_mean_squared_error: 0.6073\n",
      "Epoch 2/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: 2.9226 - root_mean_squared_error: 0.5065 - val_loss: -7.9050 - val_root_mean_squared_error: 0.4211\n",
      "Epoch 3/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -16.8925 - root_mean_squared_error: 0.3929 - val_loss: -24.3111 - val_root_mean_squared_error: 0.3780\n",
      "Epoch 4/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -31.1178 - root_mean_squared_error: 0.3736 - val_loss: -36.6165 - val_root_mean_squared_error: 0.3710\n",
      "Epoch 5/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -41.7833 - root_mean_squared_error: 0.3700 - val_loss: -46.7844 - val_root_mean_squared_error: 0.3700\n",
      "Epoch 6/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -51.3021 - root_mean_squared_error: 0.3692 - val_loss: -55.2490 - val_root_mean_squared_error: 0.3690\n",
      "Epoch 7/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -59.9894 - root_mean_squared_error: 0.3690 - val_loss: -63.6260 - val_root_mean_squared_error: 0.3689\n",
      "Epoch 8/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -67.0992 - root_mean_squared_error: 0.3691 - val_loss: -69.4570 - val_root_mean_squared_error: 0.3693\n",
      "Epoch 9/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -73.6300 - root_mean_squared_error: 0.3694 - val_loss: -75.4633 - val_root_mean_squared_error: 0.3697\n",
      "Epoch 10/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -78.9747 - root_mean_squared_error: 0.3692 - val_loss: -81.0512 - val_root_mean_squared_error: 0.3698\n",
      "Epoch 11/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -83.9942 - root_mean_squared_error: 0.3694 - val_loss: -85.4869 - val_root_mean_squared_error: 0.3690\n",
      "Epoch 12/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -87.4151 - root_mean_squared_error: 0.3693 - val_loss: -87.5682 - val_root_mean_squared_error: 0.3701\n",
      "Epoch 13/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -90.8710 - root_mean_squared_error: 0.3692 - val_loss: -93.5190 - val_root_mean_squared_error: 0.3698\n",
      "Epoch 14/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -94.1166 - root_mean_squared_error: 0.3692 - val_loss: -92.3626 - val_root_mean_squared_error: 0.3703\n",
      "Epoch 15/250\n",
      "1365/1365 [==============================] - 8s 6ms/step - loss: -97.0424 - root_mean_squared_error: 0.3692 - val_loss: -99.3429 - val_root_mean_squared_error: 0.3696\n",
      "Epoch 16/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -100.3950 - root_mean_squared_error: 0.3693 - val_loss: -101.9785 - val_root_mean_squared_error: 0.3697\n",
      "Epoch 17/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -102.8229 - root_mean_squared_error: 0.3694 - val_loss: -104.4529 - val_root_mean_squared_error: 0.3702\n",
      "Epoch 18/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -105.3953 - root_mean_squared_error: 0.3695 - val_loss: -107.4926 - val_root_mean_squared_error: 0.3709\n",
      "Epoch 19/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -107.8970 - root_mean_squared_error: 0.3691 - val_loss: -107.4036 - val_root_mean_squared_error: 0.3705\n",
      "Epoch 20/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -109.9880 - root_mean_squared_error: 0.3694 - val_loss: -108.5335 - val_root_mean_squared_error: 0.3695\n",
      "Epoch 21/250\n",
      "1365/1365 [==============================] - 8s 5ms/step - loss: -111.5287 - root_mean_squared_error: 0.3693 - val_loss: -112.1921 - val_root_mean_squared_error: 0.3698\n",
      "Epoch 22/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -113.2605 - root_mean_squared_error: 0.3693 - val_loss: -110.4945 - val_root_mean_squared_error: 0.3695\n",
      "Epoch 23/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -115.3274 - root_mean_squared_error: 0.3695 - val_loss: -114.5121 - val_root_mean_squared_error: 0.3698\n",
      "Epoch 24/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -116.8965 - root_mean_squared_error: 0.3693 - val_loss: -119.8909 - val_root_mean_squared_error: 0.3694\n",
      "Epoch 25/250\n",
      "1365/1365 [==============================] - 7s 5ms/step - loss: -118.0936 - root_mean_squared_error: 0.3692 - val_loss: -116.7850 - val_root_mean_squared_error: 0.3699\n",
      "Epoch 26/250\n",
      "  52/1365 [>.............................] - ETA: 6s - loss: -118.1146 - root_mean_squared_error: 0.3690"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "from keras.layers import Flatten, LSTM, Input\n",
    "\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.DistributionLambda(\n",
    "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return prior_model\n",
    "\n",
    "\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = keras.Sequential(\n",
    "        [\n",
    "            tfp.layers.VariableLayer(\n",
    "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "            ),\n",
    "            tfp.layers.MultivariateNormalTriL(n),\n",
    "        ]\n",
    "    )\n",
    "    return posterior_model\n",
    "\n",
    "\n",
    "def create_probablistic_bnn_model(train_size):\n",
    "    inputs = Input(shape=(42,))\n",
    "\n",
    "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
    "    features = tfp.layers.DenseVariational(\n",
    "        units=8,\n",
    "        make_prior_fn=prior,\n",
    "        make_posterior_fn=posterior,\n",
    "        kl_weight=1 / train_size,\n",
    "        activation=\"sigmoid\",\n",
    "    )(inputs)\n",
    "    \n",
    "    features = tfp.layers.DenseVariational(\n",
    "        units=8,\n",
    "        make_prior_fn=prior,\n",
    "        make_posterior_fn=posterior,\n",
    "        kl_weight=1 / train_size,\n",
    "        activation=\"sigmoid\",\n",
    "    )(features)\n",
    "\n",
    "    # Create a probabilisticå output (Normal distribution), and use the `Dense` layer\n",
    "    # to produce the parameters of the distribution.\n",
    "    # We set units=2 to learn both the mean and the variance of the Normal distribution.\n",
    "    distribution_params = layers.Dense(units=84)(features)\n",
    "    outputs = tfp.layers.IndependentNormal(42)(distribution_params)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Since the output of the model is a distribution, rather than a point estimate,\n",
    "we use the [negative loglikelihood](https://en.wikipedia.org/wiki/Likelihood_function)\n",
    "as our loss function to compute how likely to see the true data (targets) from the\n",
    "estimated distribution produced by the model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def negative_loglikelihood(targets, estimated_distribution):\n",
    "    return -estimated_distribution.log_prob(targets)\n",
    "\n",
    "\n",
    "num_epochs = 1000\n",
    "prob_bnn_model = create_probablistic_bnn_model(1000)\n",
    "\n",
    "\n",
    "prob_bnn_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=negative_loglikelihood,\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7)\n",
    "\n",
    "print(prob_bnn_model.summary())\n",
    "\n",
    "#model.fit(train_dataset,epochs=250, verbose=1, callbacks=[callback])\n",
    "\n",
    "prob_bnn_model.fit(afterstates_train, next_states_train, epochs=250, validation_split=0.1, verbose=1, callbacks=[callback], batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float32' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prob_bnn_model(afterstates_train[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprediction_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m()\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "prob_bnn_model(afterstates_train[1:1])\n",
    "prediction_distribution.mean().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_distribution = prob_bnn_model(afterstates_train[0:100])\n",
    "prediction_mean = prediction_distribution.mean().numpy()\n",
    "prediction_stdv = prediction_distribution.stddev().numpy()\n",
    "\n",
    "# The 95% CI is computed as mean ± (1.96 * stdv)\n",
    "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
    "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
    "prediction_stdv = prediction_stdv.tolist()\n",
    "\n",
    "for idx in range(1):\n",
    "    print(\n",
    "        f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
    "        f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
    "        f\"95% CI: [{round(upper[idx][0], 3)} - {round(lower[idx][0], 2)}]\"\n",
    "        f\" - Actual: {afterstates_train[idx]}\"\n",
    "    )\n",
    "    print(np.array(prediction_mean.mean(axis=0)>0.5, dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_distribution = prob_bnn_model(afterstates_test)\n",
    "prediction_mean = prediction_distribution.mean().numpy()\n",
    "count = 0\n",
    "for i in range(3960):\n",
    "    if (np.array(prediction_mean[i]>0.5, dtype=np.float32)==next_states_test[i]).all():\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(prediction_mean[i]>0.5, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_states_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (194040, 42) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [108], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianNB\n\u001b[1;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mafterstates_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m clf_pf \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[1;32m      5\u001b[0m clf_pf\u001b[38;5;241m.\u001b[39mpartial_fit(X, Y, np\u001b[38;5;241m.\u001b[39munique(Y))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/naive_bayes.py:266\u001b[0m, in \u001b[0;36mGaussianNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit Gaussian Naive Bayes according to X, y.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 266\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_fit(\n\u001b[1;32m    268\u001b[0m     X, y, np\u001b[38;5;241m.\u001b[39munique(y), _refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight\n\u001b[1;32m    269\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py:549\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    547\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m--> 549\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     out \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1143\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m-> 1143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[1;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1199\u001b[0m         )\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1204\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (194040, 42) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(afterstates_train, next_states_train)\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_distribution = model.predict(afterstates_test)\n",
    "prediction_mean = prediction_distribution.mean(axis=-1)\n",
    "count = 0\n",
    "for i in range(3960):\n",
    "    if (np.array(prediction_distribution[i]>0.5, dtype=np.float32)==next_states_test[i]).all():\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.328803  , -0.3529563 ,  0.79401225, -0.44880345,  0.35050374,\n",
       "       -0.31568593,  0.6794046 ,  1.1513491 ,  0.46755043,  0.67215884,\n",
       "        0.026644  , -0.66147274,  0.50816745, -0.01869421,  0.5954744 ,\n",
       "        0.8644304 ,  0.01966348,  0.109952  ,  0.0674424 ,  0.5989681 ,\n",
       "        1.2056735 , -0.52840537, -0.8211647 , -0.3841311 ,  0.0389117 ,\n",
       "        1.2178085 , -1.090363  ,  0.9322548 , -1.1436393 , -0.25294337,\n",
       "       -1.0193534 , -0.48505506, -1.8802378 , -0.73338646, -0.7592469 ,\n",
       "       -0.8835641 ,  1.5495107 ,  1.1624637 ,  0.2516081 ,  0.2449704 ,\n",
       "        0.51363486,  1.5921156 ], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(afterstates_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/124 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.round(model.predict(afterstates_test))\n",
    "count = 0\n",
    "for i in range(3960):\n",
    "    if (predictions[i]==next_states_test[i]).all():\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "3886\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "14\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "12\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "12\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "7\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "6\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "5\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "4\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "3\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "2\n",
      "[1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(np.array([afterstates_train[3]]))[0]\n",
    "\n",
    "new_state = {}\n",
    "\n",
    "for i in range(3960):\n",
    "    state = np.zeros(42)\n",
    "    for i in range(42):\n",
    "        state[i] = np.array(np.random.rand() < prediction[i], dtype=np.int8)\n",
    "    if not state.tobytes() in new_state:\n",
    "        new_state[state.tobytes()] = 1\n",
    "    else:\n",
    "        new_state[state.tobytes()] += 1\n",
    "        \n",
    "ns = {k: v for k, v in sorted(new_state.items(), key=lambda item: item[1], reverse=True)}\n",
    "            \n",
    "for s in ns.keys():\n",
    "    print(ns[s])\n",
    "    print(np.frombuffer(s))\n",
    "\n",
    "print(len(new_state.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "[0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "40\n",
      "[0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "16\n",
      "[0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "10\n",
      "[0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 0 0 1]\n",
      "7\n",
      "[0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "6\n",
      "[1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "5\n",
      "[0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "4\n",
      "[0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1 0 1 0 0]\n",
      "4\n",
      "[0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
      " 0 0 1 0 0]\n",
      "4\n",
      "[0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "3\n",
      "[0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 1 0 1 0 0]\n",
      "2\n",
      "[0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0]\n",
      "1\n",
      "[0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 1 0 0 0]\n",
      "1\n",
      "[0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1\n",
      " 0 0 1 0 0]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "target = afterstates_train[10]\n",
    "new_state = {}\n",
    "\n",
    "for i in range(194040):\n",
    "    if (target==afterstates_train[i]).all():\n",
    "        state = np.array(next_states_train[i], dtype=np.int8)\n",
    "        if not state.tobytes() in new_state:\n",
    "            new_state[state.tobytes()] = 1\n",
    "        else:\n",
    "            new_state[state.tobytes()] += 1\n",
    "        \n",
    "ns = {k: v for k, v in sorted(new_state.items(), key=lambda item: item[1], reverse=True)}\n",
    "            \n",
    "for s in ns.keys():\n",
    "    print(ns[s])\n",
    "    print(np.frombuffer(s, dtype=\"int8\"))\n",
    "\n",
    "print(len(new_state.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afterstates_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_states_train[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "afterstate_predictor = AfterstatePrediction(state_size=STATE_SIZE, num_mixtures=NUMBER_MIXTURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.05244306  0.17184001  0.10252205 ... -0.46421444 -1.0943612\n",
      "  -1.3317926 ]\n",
      " [ 0.09726037  0.19045556  0.37282902 ...  0.4968205  -1.7411695\n",
      "  -0.8825296 ]\n",
      " [-0.01012154  0.28047228  0.2224648  ...  0.10149805 -0.963551\n",
      "  -1.3187613 ]\n",
      " ...\n",
      " [-0.06594759  0.05624647  0.05861745 ...  0.30842698 -1.5060287\n",
      "  -0.5204618 ]\n",
      " [ 0.05493884  0.08182326  0.2528863  ... -0.06889202 -1.8719797\n",
      "  -0.89556086]\n",
      " [-0.27234328 -0.03344521  0.15791325 ... -0.48857522 -1.3138354\n",
      "  -0.6336591 ]], shape=(32, 850), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for row in test_dataset.take(1):\n",
    "    out = afterstate_predictor(row[0])\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"afterstate_prediction_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             multiple                  0 (unused)\n",
      "                                                                 \n",
      " mdn_1 (MDN)                 multiple                  36550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36,550\n",
      "Trainable params: 36,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# red_ts_predictor.compile(loss=get_mixture_loss_func(LATENT_SIZE,NUMBER_MIXTURES,red_ts_predictor.encode), optimizer=tf.keras.optimizers.Adam(),metrics=['mean_squared_error'])\n",
    "# red_ts_predictor.build(((1,78),(1,78)))\n",
    "afterstate_predictor.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = MDN.get_mixture_loss_func(STATE_SIZE,NUMBER_MIXTURES)\n",
    "optimizer = tf.keras.optimizers.Adam(.1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Test set loss: -1.3719133138656616, time elapse for current epoch: 31.3272385597229\n",
      "Epoch: 2, Test set loss: -3.8558034896850586, time elapse for current epoch: 31.34262490272522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method WeakStructRef._cleanup of WeakStructRef(HashableWeakRef(<weakref at 0x7f4ccc700b80; dead>))>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/cache_util.py\", line 151, in _cleanup\n",
      "    self._callback(self)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/cache_util.py\", line 221, in maybe_del\n",
      "    del self[key]\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m out, y \u001b[38;5;241m=\u001b[39m forward_pass(afterstate_predictor, test_x, test_y)\n\u001b[0;32m---> 24\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss(loss_val)\n\u001b[1;32m     26\u001b[0m loss_result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/tf/Cage/Notebooks/MDN.py:110\u001b[0m, in \u001b[0;36mget_mixture_loss_func.<locals>.mdn_loss_func\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    107\u001b[0m sigs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msplit(out_sigma, num_or_size_splits\u001b[38;5;241m=\u001b[39mcomponent_splits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    108\u001b[0m coll \u001b[38;5;241m=\u001b[39m [tfd\u001b[38;5;241m.\u001b[39mMultivariateNormalDiag(loc\u001b[38;5;241m=\u001b[39mloc, scale_diag\u001b[38;5;241m=\u001b[39mscale) \u001b[38;5;28;01mfor\u001b[39;00m loc, scale\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(mus, sigs)]\n\u001b[0;32m--> 110\u001b[0m mixture \u001b[38;5;241m=\u001b[39m \u001b[43mtfd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMixture\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m mixture\u001b[38;5;241m.\u001b[39mlog_prob(y_true)\n\u001b[1;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnegative(loss)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/distribution.py:342\u001b[0m, in \u001b[0;36m_DistributionMeta.__new__.<locals>.wrapped_init\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to have things set in `self` before `__init__` is\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# called, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m \u001b[43mdefault_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# Note: if we ever want to override things set in `self` by subclass\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# `__init__`, here is the place to do it.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m   \u001b[38;5;66;03m# We prefer subclasses will set `parameters = dict(locals())` because\u001b[39;00m\n\u001b[1;32m    347\u001b[0m   \u001b[38;5;66;03m# this has nearly zero overhead. However, failing to do this, we will\u001b[39;00m\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;66;03m# resolve the input arguments dynamically and only when needed.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/mixture.py:155\u001b[0m, in \u001b[0;36m_Mixture.__init__\u001b[0;34m(self, cat, components, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    152\u001b[0m static_batch_shape \u001b[38;5;241m=\u001b[39m cat\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m di, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(components):\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensorshape_util\u001b[38;5;241m.\u001b[39mis_compatible_with(static_batch_shape,\n\u001b[0;32m--> 155\u001b[0m                                              \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_shape\u001b[49m):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponents[\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] batch shape must be compatible with cat \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape and other component batch shapes (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    159\u001b[0m             di, static_batch_shape, d\u001b[38;5;241m.\u001b[39mbatch_shape))\n\u001b[1;32m    160\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensorshape_util\u001b[38;5;241m.\u001b[39mis_compatible_with(static_event_shape,\n\u001b[1;32m    161\u001b[0m                                              d\u001b[38;5;241m.\u001b[39mevent_shape):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/distribution.py:1079\u001b[0m, in \u001b[0;36mDistribution.batch_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shape of a single sample from a single event index as a `TensorShape`.\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mMay be partially defined or unknown.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m  batch_shape: `TensorShape`, possibly unknown.\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__cached_batch_shape\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;66;03m# Cache the batch shape so that it's only inferred once. This is safe\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m   \u001b[38;5;66;03m# because runtime changes to parameter shapes can only affect\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m   \u001b[38;5;66;03m# `batch_shape_tensor`, never `batch_shape`.\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m   batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m   \u001b[38;5;66;03m# See comment in `batch_shape_tensor()` on structured batch shapes. If\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m   \u001b[38;5;66;03m# `_batch_shape()` is a `tf.TensorShape` instance or a flat list/tuple\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m   \u001b[38;5;66;03m# that does not contain `tf.TensorShape`s, we infer that it is not\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# structured.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(batch_shape, tf\u001b[38;5;241m.\u001b[39mTensorShape)\n\u001b[1;32m   1086\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, tf\u001b[38;5;241m.\u001b[39mTensorShape)\n\u001b[1;32m   1087\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m path, s \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten_with_tuple_paths(batch_shape))):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/transformed_distribution.py:299\u001b[0m, in \u001b[0;36m_TransformedDistribution._batch_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 299\u001b[0m   batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_shape\u001b[49m\n\u001b[1;32m    300\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mis_nested(batch_shape) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_joint:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# Pass-through rudimentary support for JDs with structured batch shape.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# TODO(b/194742372): remove support for structured batch shape.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(batch_shape))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/distribution.py:1079\u001b[0m, in \u001b[0;36mDistribution.batch_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shape of a single sample from a single event index as a `TensorShape`.\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mMay be partially defined or unknown.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m  batch_shape: `TensorShape`, possibly unknown.\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__cached_batch_shape\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;66;03m# Cache the batch shape so that it's only inferred once. This is safe\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m   \u001b[38;5;66;03m# because runtime changes to parameter shapes can only affect\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m   \u001b[38;5;66;03m# `batch_shape_tensor`, never `batch_shape`.\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m   batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m   \u001b[38;5;66;03m# See comment in `batch_shape_tensor()` on structured batch shapes. If\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m   \u001b[38;5;66;03m# `_batch_shape()` is a `tf.TensorShape` instance or a flat list/tuple\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m   \u001b[38;5;66;03m# that does not contain `tf.TensorShape`s, we infer that it is not\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# structured.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(batch_shape, tf\u001b[38;5;241m.\u001b[39mTensorShape)\n\u001b[1;32m   1086\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, tf\u001b[38;5;241m.\u001b[39mTensorShape)\n\u001b[1;32m   1087\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m path, s \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten_with_tuple_paths(batch_shape))):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/distribution.py:1057\u001b[0m, in \u001b[0;36mDistribution._batch_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Infers static batch shape from parameters.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03mThe overall batch shape is inferred by broadcasting the batch shapes of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    be partially defined or unknown.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1057\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbatch_shape_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minferred_batch_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m   \u001b[38;5;66;03m# If a distribution doesn't implement `_parameter_properties` or its own\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m   \u001b[38;5;66;03m# `_batch_shape` method, we can only return the most general shape.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:68\u001b[0m, in \u001b[0;36minferred_batch_shape\u001b[0;34m(batch_object, bijector_x_event_ndims)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minferred_batch_shape\u001b[39m(batch_object, bijector_x_event_ndims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Infers an object's batch shape from its  parameters.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m  Each parameter contributes a batch shape of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        be partially defined or unknown.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m   batch_shapes \u001b[38;5;241m=\u001b[39m \u001b[43mmap_fn_over_parameters_with_event_ndims\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m      \u001b[49m\u001b[43mget_batch_shape_part\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrequire_static\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbijector_x_event_ndims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbijector_x_event_ndims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(tf\u001b[38;5;241m.\u001b[39mbroadcast_static_shape,\n\u001b[1;32m     74\u001b[0m                           tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(batch_shapes),\n\u001b[1;32m     75\u001b[0m                           tf\u001b[38;5;241m.\u001b[39mTensorShape([]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:367\u001b[0m, in \u001b[0;36mmap_fn_over_parameters_with_event_ndims\u001b[0;34m(batch_object, fn, bijector_x_event_ndims, require_static, **parameter_kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (properties\u001b[38;5;241m.\u001b[39mis_tensor\n\u001b[1;32m    362\u001b[0m           \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(param)\n\u001b[1;32m    363\u001b[0m           \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mis_nested(param_event_ndims)):\n\u001b[1;32m    364\u001b[0m       \u001b[38;5;66;03m# As a last resort, try an explicit conversion.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m       param \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconvert_nonref_to_tensor(param, name\u001b[38;5;241m=\u001b[39mparam_name)\n\u001b[0;32m--> 367\u001b[0m   results[param_name] \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_event_ndims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1435\u001b[0m, in \u001b[0;36mmap_structure_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1435\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_structure_with_tuple_paths_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Discards the path arg.\u001b[39;49;00m\n\u001b[1;32m   1438\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1535\u001b[0m, in \u001b[0;36mmap_structure_with_tuple_paths_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1527\u001b[0m     flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m         shallow_tree,\n\u001b[1;32m   1529\u001b[0m         input_tree,\n\u001b[1;32m   1530\u001b[0m         check_types,\n\u001b[1;32m   1531\u001b[0m         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites) \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1532\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1533\u001b[0m     path\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn))\n\u001b[0;32m-> 1535\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1536\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1537\u001b[0m ]\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(structure\u001b[38;5;241m=\u001b[39mshallow_tree, flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1539\u001b[0m                         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1536\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1526\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1527\u001b[0m     flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m         shallow_tree,\n\u001b[1;32m   1529\u001b[0m         input_tree,\n\u001b[1;32m   1530\u001b[0m         check_types,\n\u001b[1;32m   1531\u001b[0m         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites) \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1532\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1533\u001b[0m     path\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn))\n\u001b[1;32m   1535\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1536\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1537\u001b[0m ]\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(structure\u001b[38;5;241m=\u001b[39mshallow_tree, flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1539\u001b[0m                         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1437\u001b[0m, in \u001b[0;36mmap_structure_up_to.<locals>.<lambda>\u001b[0;34m(_, *values)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m map_structure_with_tuple_paths_up_to(\n\u001b[1;32m   1436\u001b[0m       shallow_tree,\n\u001b[0;32m-> 1437\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m _, \u001b[38;5;241m*\u001b[39mvalues: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Discards the path arg.\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m       \u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m   1439\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:156\u001b[0m, in \u001b[0;36mget_batch_shape_part\u001b[0;34m(x, event_ndims)\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Backwards compatibility with bijector-like instances that don't\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# inherit from tfb.Bijector (e.g., Distrax bijectors) and/or don't\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# implement `_parameter_properties`.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape([])\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_shape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# `x` is a distribution or linear operator.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m   base_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m    158\u001b[0m   \u001b[38;5;66;03m# Hack to attempt to collapse non-autobatched JDs to a coherent batch shape.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m   \u001b[38;5;66;03m# Distrax distributions have tuple-valued batch shapes, which are\u001b[39;00m\n\u001b[1;32m    160\u001b[0m   \u001b[38;5;66;03m# always spuriously 'nested', so to avoid confusion we check that the\u001b[39;00m\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;66;03m# distribution is actually a joint distribution (Distrax\u001b[39;00m\n\u001b[1;32m    162\u001b[0m   \u001b[38;5;66;03m# distributions are always single-part).\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/distribution.py:1079\u001b[0m, in \u001b[0;36mDistribution.batch_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Shape of a single sample from a single event index as a `TensorShape`.\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mMay be partially defined or unknown.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m  batch_shape: `TensorShape`, possibly unknown.\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__cached_batch_shape\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;66;03m# Cache the batch shape so that it's only inferred once. This is safe\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m   \u001b[38;5;66;03m# because runtime changes to parameter shapes can only affect\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m   \u001b[38;5;66;03m# `batch_shape_tensor`, never `batch_shape`.\u001b[39;00m\n\u001b[0;32m-> 1079\u001b[0m   batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m   \u001b[38;5;66;03m# See comment in `batch_shape_tensor()` on structured batch shapes. If\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m   \u001b[38;5;66;03m# `_batch_shape()` is a `tf.TensorShape` instance or a flat list/tuple\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m   \u001b[38;5;66;03m# that does not contain `tf.TensorShape`s, we infer that it is not\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# structured.\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(batch_shape, tf\u001b[38;5;241m.\u001b[39mTensorShape)\n\u001b[1;32m   1086\u001b[0m       \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, tf\u001b[38;5;241m.\u001b[39mTensorShape)\n\u001b[1;32m   1087\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m path, s \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten_with_tuple_paths(batch_shape))):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/distributions/distribution.py:1057\u001b[0m, in \u001b[0;36mDistribution._batch_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Infers static batch shape from parameters.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03mThe overall batch shape is inferred by broadcasting the batch shapes of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    be partially defined or unknown.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1057\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbatch_shape_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minferred_batch_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m   \u001b[38;5;66;03m# If a distribution doesn't implement `_parameter_properties` or its own\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m   \u001b[38;5;66;03m# `_batch_shape` method, we can only return the most general shape.\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:68\u001b[0m, in \u001b[0;36minferred_batch_shape\u001b[0;34m(batch_object, bijector_x_event_ndims)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minferred_batch_shape\u001b[39m(batch_object, bijector_x_event_ndims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Infers an object's batch shape from its  parameters.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m  Each parameter contributes a batch shape of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        be partially defined or unknown.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m   batch_shapes \u001b[38;5;241m=\u001b[39m \u001b[43mmap_fn_over_parameters_with_event_ndims\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m      \u001b[49m\u001b[43mget_batch_shape_part\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m      \u001b[49m\u001b[43mrequire_static\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbijector_x_event_ndims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbijector_x_event_ndims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mreduce(tf\u001b[38;5;241m.\u001b[39mbroadcast_static_shape,\n\u001b[1;32m     74\u001b[0m                           tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(batch_shapes),\n\u001b[1;32m     75\u001b[0m                           tf\u001b[38;5;241m.\u001b[39mTensorShape([]))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:367\u001b[0m, in \u001b[0;36mmap_fn_over_parameters_with_event_ndims\u001b[0;34m(batch_object, fn, bijector_x_event_ndims, require_static, **parameter_kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (properties\u001b[38;5;241m.\u001b[39mis_tensor\n\u001b[1;32m    362\u001b[0m           \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mis_tensor(param)\n\u001b[1;32m    363\u001b[0m           \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mis_nested(param_event_ndims)):\n\u001b[1;32m    364\u001b[0m       \u001b[38;5;66;03m# As a last resort, try an explicit conversion.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m       param \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconvert_nonref_to_tensor(param, name\u001b[38;5;241m=\u001b[39mparam_name)\n\u001b[0;32m--> 367\u001b[0m   results[param_name] \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_event_ndims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1435\u001b[0m, in \u001b[0;36mmap_structure_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1435\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_structure_with_tuple_paths_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Discards the path arg.\u001b[39;49;00m\n\u001b[1;32m   1438\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1535\u001b[0m, in \u001b[0;36mmap_structure_with_tuple_paths_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1526\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1527\u001b[0m     flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m         shallow_tree,\n\u001b[1;32m   1529\u001b[0m         input_tree,\n\u001b[1;32m   1530\u001b[0m         check_types,\n\u001b[1;32m   1531\u001b[0m         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites) \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1532\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1533\u001b[0m     path\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn))\n\u001b[0;32m-> 1535\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1536\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1537\u001b[0m ]\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(structure\u001b[38;5;241m=\u001b[39mshallow_tree, flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1539\u001b[0m                         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1536\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1526\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1527\u001b[0m     flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m         shallow_tree,\n\u001b[1;32m   1529\u001b[0m         input_tree,\n\u001b[1;32m   1530\u001b[0m         check_types,\n\u001b[1;32m   1531\u001b[0m         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites) \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[1;32m   1532\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1533\u001b[0m     path\n\u001b[1;32m   1534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _yield_flat_up_to(shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn))\n\u001b[1;32m   1535\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1536\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1537\u001b[0m ]\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(structure\u001b[38;5;241m=\u001b[39mshallow_tree, flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1539\u001b[0m                         expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:1437\u001b[0m, in \u001b[0;36mmap_structure_up_to.<locals>.<lambda>\u001b[0;34m(_, *values)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \n\u001b[1;32m   1365\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m map_structure_with_tuple_paths_up_to(\n\u001b[1;32m   1436\u001b[0m       shallow_tree,\n\u001b[0;32m-> 1437\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m _, \u001b[38;5;241m*\u001b[39mvalues: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Discards the path arg.\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m       \u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m   1439\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:172\u001b[0m, in \u001b[0;36mget_batch_shape_part\u001b[0;34m(x, event_ndims)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;66;03m# `x` is a Python list, tuple, or literal.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   base_shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensorShape(np\u001b[38;5;241m.\u001b[39marray(x)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_truncate_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_ndims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/internal/batch_shape_lib.py:196\u001b[0m, in \u001b[0;36m_truncate_shape\u001b[0;34m(shape, rightmost_ndims_to_truncate)\u001b[0m\n\u001b[1;32m    194\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m event_ndims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\n\u001b[1;32m    197\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Don't try to slice away more ndims than the parameter\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# actually has, if that's fewer than `event_ndims` (i.e.,\u001b[39;49;00m\n\u001b[1;32m    199\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# if it relies on broadcasting).\u001b[39;49;00m\n\u001b[1;32m    200\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrightmost_ndims_to_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py:901\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of a dimension or a shape, depending on the key.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m      the step is set.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m    903\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m TensorShape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims[key])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# EPOCHS = 500\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    for train_x, train_y in train_dataset:\n",
    "        #if (count %1000) == 0:\n",
    "        #    print(f\"{count}={count*batch_size} samples\")\n",
    "        train_step(afterstate_predictor, train_x, train_y, optimizer, loss_func)\n",
    "        count += 1\n",
    "    end_time = time.time()\n",
    "\n",
    "    \n",
    "    total_matches = 0\n",
    "    total = 0\n",
    "    nodes = 0\n",
    "    sum_diffs_sqrd = 0\n",
    "    state_pred_pairs = []\n",
    "    state_pred_pair_tree_vis = []\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    loss_result, total = 0, 0\n",
    "    for test_x, test_y in test_dataset.take(1000):#.take(10):#test_dataset:\n",
    "        total += 1\n",
    "        out, y = forward_pass(afterstate_predictor, test_x, test_y)\n",
    "        loss_val = loss_func(y, out)\n",
    "        loss(loss_val)\n",
    "        loss_result += loss.result()\n",
    "        #     print(f\"accuracy = {total_matches}/{total} = {total_matches/total}, \\nmean of squared diffs = {sum_diffs_sqrd}/{nodes}={sum_diffs_sqrd/nodes}\\npercentage wrong = ({sum_diffs_sqrd}/{2})/({nodes}/{3})={(sum_diffs_sqrd/2)/(nodes/3)}\")\n",
    "    print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, loss_result/total, end_time - start_time))\n",
    "        \n",
    "#         sampled_out = MDN.sample_from_output(out[0].numpy(), output_dim=LATENT_SIZE, num_mixes=NUMBER_MIXTURES)\n",
    "#         pred_oh, y_oh = decode_zs(red_ts_predictor, sampled_out, y_encoded)\n",
    "        \n",
    "        \n",
    "#         state_pred_pairs.append([y_oh, pred_oh])\n",
    "# #         state_pred_pair_tree_vis.append([TrueStateTreeGraphViz(y_oh), TrueStateTreeGraphViz(pred_oh)])\n",
    "# #         loss(compute_loss(red_ts_predictor, test_x, test_y, loss_func))\n",
    "#         diffs = np.rint(y_oh.numpy()) - np.rint(pred_oh.numpy())\n",
    "#     #     diffs = get_state_diff(true_state_model,test_x)\n",
    "#         nodes += len(diffs.flatten())\n",
    "#         diffs_sqrd = np.sum(diffs*diffs)\n",
    "#         sum_diffs_sqrd += diffs_sqrd\n",
    "#         if not diffs_sqrd >0:\n",
    "#     #       print(diffs)\n",
    "#     #     else:\n",
    "#           total_matches += 1\n",
    "#     #       print(\"Match\")\n",
    "    #       print(diffs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre_ts_encoded = red_ts_predictor.ts_vae.encode(np.zeros((1,78),dtype=np.float32))\n",
    "# red_ts_predictor.ts_dense(pre_ts_encoded)\n",
    "\n",
    "np.array(sample_from_output(afterstate_predictor(np.zeros((1,42)))[0].numpy(), 42, 5)>0.5, dtype=np.int8)\n",
    "\n",
    "#afterstate_predictor.save('models/afterstate_predictor',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "m2 = tf.keras.models.load_model(\n",
    "    'models/afterstate_predictor',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Value for attr 'TI' of float is not in the list of allowed values: uint8, int8, int32, int64\n\t; NodeDef: {{node OneHot}}; Op<name=OneHot; signature=indices:TI, depth:int32, on_value:T, off_value:T -> output:T; attr=axis:int,default=-1; attr=T:type; attr=TI:type,default=DT_INT64,allowed=[DT_UINT8, DT_INT8, DT_INT32, DT_INT64]> [Op:OneHot]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m num_eval_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (test_x, test_y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m3000\u001b[39m)):\u001b[38;5;66;03m#.take(10):#test_dataset:\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     pre_ts \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m,(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     18\u001b[0m     blue_ts \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(tf\u001b[38;5;241m.\u001b[39mone_hot(test_x[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m3\u001b[39m),(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     20\u001b[0m     out, y_encoded \u001b[38;5;241m=\u001b[39m forward_pass(m2, test_x, test_y)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Value for attr 'TI' of float is not in the list of allowed values: uint8, int8, int32, int64\n\t; NodeDef: {{node OneHot}}; Op<name=OneHot; signature=indices:TI, depth:int32, on_value:T, off_value:T -> output:T; attr=axis:int,default=-1; attr=T:type; attr=TI:type,default=DT_INT64,allowed=[DT_UINT8, DT_INT8, DT_INT32, DT_INT64]> [Op:OneHot]"
     ]
    }
   ],
   "source": [
    "total_matches = 0\n",
    "total = 0\n",
    "nodes = 0\n",
    "sum_diffs_sqrd = 0\n",
    "state_pred_pairs = []\n",
    "state_pred_pair_tree_vis = []\n",
    "state_pred_samples_correct = []\n",
    "\n",
    "red_change_test_indices = []\n",
    "no_change_indices = []\n",
    "\n",
    "loss = tf.keras.metrics.Mean()\n",
    "\n",
    "num_eval_samples = 10\n",
    "\n",
    "for i, (test_x, test_y) in enumerate(test_dataset.take(3000)):#.take(10):#test_dataset:\n",
    "    pre_ts = tf.reshape(tf.one_hot(test_x[0],3),(-1,2,3))\n",
    "    blue_ts = tf.reshape(tf.one_hot(test_x[1],3),(-1,2,3))\n",
    "    \n",
    "    out, y_encoded = forward_pass(m2, test_x, test_y)\n",
    "#     y_oh = decode_z(m2, y_encoded)\n",
    "    \n",
    "    y_oh = tf.one_hot(tf.reshape(test_y,(-1,2)),depth=3)\n",
    "    \n",
    "#     if not ONLY_MEASURE_CHANGES or np.any(blue_ts != y_oh):\n",
    "\n",
    "    if np.any(blue_ts != y_oh):\n",
    "        red_change_test_indices.append(i)\n",
    "    else:\n",
    "        no_change_indices.append(i)\n",
    "        \n",
    "    loss_val = loss_func(y_encoded, out)\n",
    "    loss(loss_val)\n",
    "\n",
    "    y_tree_vis = TrueStateTreeGraphViz(y_oh)\n",
    "\n",
    "    predictions = []\n",
    "    pred_tree_vis = []\n",
    "    pred_corrects = []\n",
    "    for i in range(num_eval_samples):\n",
    "        total += 1\n",
    "        sampled_out = MDN.sample_from_output(out[0].numpy(), output_dim=STATE_SIZE, num_mixes=NUMBER_MIXTURES)\n",
    "        pred_oh = decode_z(m2, sampled_out)\n",
    "\n",
    "        predictions.append(pred_oh)\n",
    "        pred_tree_vis.append(TrueStateTreeGraphViz(pred_oh))\n",
    "\n",
    "        diffs = np.rint(y_oh.numpy()) - np.rint(pred_oh.numpy())\n",
    "        diffs_sqrd = np.sum(diffs*diffs)\n",
    "        sum_diffs_sqrd += diffs_sqrd\n",
    "\n",
    "        nodes += len(diffs.flatten())\n",
    "        pred_correct = not diffs_sqrd >0\n",
    "        if pred_correct:\n",
    "            total_matches += 1\n",
    "        pred_corrects.append(pred_correct)\n",
    "\n",
    "\n",
    "\n",
    "    state_pred_pairs.append([y_oh, predictions])\n",
    "\n",
    "    state_pred_pair_tree_vis.append([TrueStateTreeGraphViz(pre_ts),\n",
    "                                     TrueStateTreeGraphViz(blue_ts),\n",
    "                                     y_tree_vis,\n",
    "                                     pred_tree_vis])\n",
    "\n",
    "    state_pred_samples_correct.append(pred_corrects)\n",
    "\n",
    "state_pred_samples_correct = np.array(state_pred_samples_correct)\n",
    "\n",
    "#         state_pred_pairs.append([y_oh, pred_oh])\n",
    "#     state_pred_pair_tree_vis.append([TrueStateTreeGraphViz(y_oh), TrueStateTreeGraphViz(pred_oh)])\n",
    "#         loss(compute_loss(m2, test_x, test_y, loss_func))\n",
    "#     diffs = np.rint(y_oh.numpy()) - np.rint(pred_oh.numpy())\n",
    "#     diffs = get_state_diff(true_state_model,test_x)\n",
    "#     nodes += len(diffs.flatten())\n",
    "#     diffs_sqrd = np.sum(diffs*diffs)\n",
    "#     sum_diffs_sqrd += diffs_sqrd\n",
    "#     if not diffs_sqrd >0:\n",
    "# #       print(diffs)\n",
    "# #     else:\n",
    "#       total_matches += 1\n",
    "#       print(\"Match\")\n",
    "#       print(diffs)\n",
    "\n",
    "loss = loss.result()\n",
    "display.clear_output(wait=False)\n",
    "print(f\"accuracy = {total_matches}/{total} = {total_matches/total}, \\nmean of squared diffs = {sum_diffs_sqrd}/{nodes}={sum_diffs_sqrd/nodes}\\npercentage wrong = ({sum_diffs_sqrd}/{2})/({nodes}/{3})={(sum_diffs_sqrd/2)/(nodes/3)}\")\n",
    "# print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'\n",
    "#     .format(epoch, loss, end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correct_pred_counts = np.sum(state_pred_samples_correct[no_change_indices],axis=1)\n",
    "\n",
    "correct_pred_counts_change_only = np.sum(state_pred_samples_correct[red_change_test_indices], axis=1)\n",
    "\n",
    "# print(f\"Correct prediction frequencies: {correct_pred_counts}\")\n",
    "\n",
    "plt.hist([correct_pred_counts,correct_pred_counts_change_only],num_eval_samples+1,density=True, stacked=True, label=[\"No state change\", \"Changes only\"])\n",
    "# plt.hist(correct_pred_counts_change_only,num_eval_samples+1,density=True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Count of correct predictions in 10 samples')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import true_state_viewer\n",
    "reload(true_state_viewer)\n",
    "from true_state_viewer import display_tree_red_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree_red_preds(state_pred_pair_tree_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
