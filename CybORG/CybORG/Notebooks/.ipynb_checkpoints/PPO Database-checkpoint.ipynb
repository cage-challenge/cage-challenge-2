{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "import numpy as np\n",
    "from ray import air, tune\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "register_env(name=\"CybORG\", env_creator=env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-12-16 12:30:40</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:01.76        </td></tr>\n",
       "<tr><td>Memory:      </td><td>37.2/125.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 31.0/40 CPUs, 1.0/1 GPUs, 0.0/74.93 GiB heap, 0.0/36.1 GiB objects (0.0/1.0 accelerator_type:V100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_worker\n",
       "s</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CybORG_4ea3f_00000</td><td>RUNNING </td><td>172.28.0.2:33208</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         32.7645</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-520.983</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">              -109.8</td><td style=\"text-align: right;\">             -1145.8</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:29:44,930\tINFO algorithm.py:2303 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:29:44,930\tWARNING ppo.py:351 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=30 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 100.\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:29:44,930\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:29:44,932\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=33238)\u001b[0m 2022-12-16 12:29:55,471\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:30:02,105\tINFO trainable.py:164 -- Trainable.setup took 17.177 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:30:02,107\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname    </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                              </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip   </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_recreated_workers</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                           </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                               </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CybORG_4ea3f_00000</td><td style=\"text-align: right;\">                  18000</td><td>{&#x27;num_env_steps_sampled&#x27;: 18000, &#x27;num_env_steps_trained&#x27;: 18000, &#x27;num_agent_steps_sampled&#x27;: 18000, &#x27;num_agent_steps_trained&#x27;: 18000}</td><td>{}              </td><td>2022-12-16_12-30-35</td><td>False </td><td style=\"text-align: right;\">               100</td><td>{}             </td><td style=\"text-align: right;\">              -109.8</td><td style=\"text-align: right;\">             -520.983</td><td style=\"text-align: right;\">             -1145.8</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">             180</td><td>d733ef48241744f583e57d5b6c039e0d</td><td>01589170c3ff</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.20000000298023224, &#x27;cur_lr&#x27;: 0.0005000000237487257, &#x27;total_loss&#x27;: 8.900472, &#x27;policy_loss&#x27;: -0.2132334, &#x27;vf_loss&#x27;: 9.108389, &#x27;vf_explained_var&#x27;: -0.025767138, &#x27;kl&#x27;: 0.026581803, &#x27;entropy&#x27;: 4.824505, &#x27;entropy_coeff&#x27;: 0.0, &#x27;model&#x27;: {}}, &#x27;train&#x27;: None}}, &#x27;num_env_steps_sampled&#x27;: 18000, &#x27;num_env_steps_trained&#x27;: 18000, &#x27;num_agent_steps_sampled&#x27;: 18000, &#x27;num_agent_steps_trained&#x27;: 18000}</td><td style=\"text-align: right;\">                         6</td><td>172.28.0.2</td><td style=\"text-align: right;\">                    18000</td><td style=\"text-align: right;\">                    18000</td><td style=\"text-align: right;\">                  18000</td><td style=\"text-align: right;\">                             3000</td><td style=\"text-align: right;\">                  18000</td><td style=\"text-align: right;\">                             3000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   30</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         3000</td><td>{&#x27;cpu_util_percent&#x27;: 32.17142857142857, &#x27;ram_util_percent&#x27;: 29.599999999999998}</td><td style=\"text-align: right;\">33208</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 1.70938626683753, &#x27;mean_inference_ms&#x27;: 1.874125006161448, &#x27;mean_action_processing_ms&#x27;: 0.1553102814265939, &#x27;mean_env_wait_ms&#x27;: 11.082980257571654, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -109.79999999999987, &#x27;episode_reward_min&#x27;: -1145.8000000000006, &#x27;episode_reward_mean&#x27;: -520.9830000000003, &#x27;episode_len_mean&#x27;: 100.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 30, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-370.2, -1035.5, -277.6999999999998, -151.79999999999978, -782.6000000000007, -473.8000000000005, -1034.9000000000005, -288.79999999999995, -1039.8000000000013, -792.8000000000013, -472.8000000000008, -283.6, -719.8000000000014, -1091.2000000000007, -378.80000000000064, -790.7000000000014, -289.7999999999999, -870.7000000000008, -317.8000000000002, -1037.7000000000005, -1015.9000000000001, -1136.4, -122.79999999999977, -1145.8000000000006, -1011.2000000000008, -697.8000000000013, -243.6, -293.7999999999999, -303.8, -1074.8000000000009, -151.3, -223.2999999999998, -1138.8999999999999, -454.1, -218.59999999999968, -290.79999999999995, -295.8000000000005, -109.79999999999987, -514.7000000000014, -1017.0999999999999, -298.2, -1061.800000000001, -357.70000000000044, -266.7999999999998, -245.5, -1142.8, -282.2, -1066.0000000000005, -283.6999999999999, -799.5000000000013, -471.8000000000013, -822.8000000000014, -337.79999999999984, -868.5, -394.80000000000075, -1103.7000000000007, -274.39999999999986, -275.99999999999994, -182.7, -494.6, -236.9, -219.7999999999996, -448.70000000000147, -370.8000000000013, -498.8000000000013, -898.8000000000011, -1065.700000000001, -233.29999999999995, -199.69999999999956, -386.0, -637.7, -309.80000000000035, -469.0, -495.0000000000001, -231.4, -560.8000000000005, -369.8000000000007, -264.8999999999999, -361.8000000000009, -199.79999999999967, -253.79999999999964, -237.79999999999964, -563.0, -262.1, -979.7000000000013, -1061.700000000001, -858.8000000000014, -235.09999999999982, -319.80000000000035, -462.80000000000143, -542.8000000000014, -306.3, -199.79999999999967, -289.60000000000036, -473.70000000000147, -288.4999999999998, -267.6999999999999, -506.8000000000011, -274.7999999999998, -262.7999999999997], &#x27;episode_lengths&#x27;: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 1.70938626683753, &#x27;mean_inference_ms&#x27;: 1.874125006161448, &#x27;mean_action_processing_ms&#x27;: 0.1553102814265939, &#x27;mean_env_wait_ms&#x27;: 11.082980257571654, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             32.7645</td><td style=\"text-align: right;\">           5.44944</td><td style=\"text-align: right;\">       32.7645</td><td>{&#x27;training_iteration_time_ms&#x27;: 5446.38, &#x27;load_time_ms&#x27;: 7.924, &#x27;load_throughput&#x27;: 378600.445, &#x27;learn_time_ms&#x27;: 3752.862, &#x27;learn_throughput&#x27;: 799.39, &#x27;synch_weights_time_ms&#x27;: 11.747}</td><td style=\"text-align: right;\"> 1671193835</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            18000</td><td style=\"text-align: right;\">                   6</td><td>4ea3f_00000</td><td style=\"text-align: right;\">      17.1936</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 12:30:36,145\tWARNING tune.py:705 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m 2022-12-16 12:30:40,302\tERROR worker.py:763 -- Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1032, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"python/ray/_raylet.pyx\", line 812, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"python/ray/_raylet.pyx\", line 852, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"python/ray/_raylet.pyx\", line 859, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"python/ray/_raylet.pyx\", line 863, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"python/ray/_raylet.pyx\", line 810, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/_private/function_manager.py\", line 674, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/tune/trainable/trainable.py\", line 352, in train\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 772, in step\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     results, train_iter_ctx = self._run_one_training_iteration()\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/algorithm.py\", line 2948, in _run_one_training_iteration\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     results = self.training_step()\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/algorithms/ppo/ppo.py\", line 421, in training_step\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     train_results = multi_gpu_train_one_step(self, train_batch)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/execution/train_ops.py\", line 180, in multi_gpu_train_one_step\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     learner_info_builder.add_learn_on_batch_results(results, policy_id)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/utils/metrics/learner_info.py\", line 47, in add_learn_on_batch_results\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     tree.map_structure_with_path(\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/tree/__init__.py\", line 469, in map_structure_with_path\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return map_structure_with_path_up_to(structures[0], func, *structures,\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/tree/__init__.py\", line 759, in map_structure_with_path_up_to\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     results.append(func(*path_and_values))\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/utils/metrics/learner_info.py\", line 48, in <lambda>\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     lambda p, *s: _all_tower_reduce(p, *s),\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/rllib/utils/metrics/learner_info.py\", line 112, in _all_tower_reduce\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     return np.nanmean(tower_data)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"<__array_function__ internals>\", line 180, in nanmean\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/nanfunctions.py\", line 1033, in nanmean\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     arr, mask = _replace_nan(a, 0)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/numpy/lib/nanfunctions.py\", line 102, in _replace_nan\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     mask = np.isnan(a)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/ray/_private/worker.py\", line 760, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(PPO pid=33208)\u001b[0m SystemExit: 1\n",
      "2022-12-16 12:30:40,412\tERROR tune.py:773 -- Trials did not complete: [PPO_CybORG_4ea3f_00000]\n",
      "2022-12-16 12:30:40,413\tINFO tune.py:777 -- Total run time: 62.00 seconds (61.76 seconds for the tuning loop).\n",
      "2022-12-16 12:30:40,414\tWARNING tune.py:783 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.result_grid.ResultGrid at 0x7efc60c33160>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tune.Tuner(\n",
    "        \"PPO\",\n",
    "        run_config=air.RunConfig(\n",
    "            stop={\"timesteps_total\": 2e6},\n",
    "            local_dir='results/APPO', name=\"explore2\",\n",
    "            checkpoint_config=air.CheckpointConfig(\n",
    "                checkpoint_frequency=500, \n",
    "            ),\n",
    "        ),\n",
    "        param_space={\n",
    "            # CC3 specific.\n",
    "            \"env\": \"CybORG\",\n",
    "            # General\n",
    "            \"num_gpus\": 1,\n",
    "            \"num_workers\": 30,\n",
    "            \"horizon\": 100,\n",
    "            \"num_envs_per_worker\": 1,\n",
    "            #\"exploration_config\": tune.grid_search([{\"type\": \"RE3\",\n",
    "            #    \"embeds_dim\": 128,\n",
    "            #    \"beta_schedule\": \"constant\",\n",
    "            #    \"sub_exploration\": {\n",
    "            #        \"type\": \"StochasticSampling\",\n",
    "            #},}, \n",
    "            \"exploration_config\": {\"type\": \"StochasticSampling\"},\n",
    "            #algo params\n",
    "            \"train_batch_size\": 3000,\n",
    "            \"lr\": 0.0005,\n",
    "            \"gamma\": 0.95,\n",
    "            \"framework\": 'tf',\n",
    "            \"model\": {\n",
    "                    \"fcnet_hiddens\": [512, 512],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                    #'use_lstm': True,\n",
    "                    #'max_seq_len': 5,\n",
    "                },\n",
    "            \"output\": \"dataset\",\n",
    "            \"output_config\": {\n",
    "                \"format\": \"json\",\n",
    "                \"path\": \"/logs/\"},\n",
    "            \"output_compress_columns\": ['prev_actions', 'prev_rewards', 'dones', 't', 'eps_id', 'unroll_id', 'agent_index', 'action_prob', 'action_logp', 'action_dist_inputs', 'advantages', 'value_targets']\n",
    "        },\n",
    "    ).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  5 12:03:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.03   Driver Version: 450.119.03   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    38W / 250W |   2167MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030630571070921465"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = np.array([0.9997, 0.0001, 0.0001, 0.0001])\n",
    "entropy = -np.sum(dist * np.log(dist))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6949892486343404"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = np.array([0.5, 0.5, 0.0001, 0.0001])\n",
    "entropy = -np.sum(dist * np.log(dist))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "entropy = -np.sum(dist * np.log(dist))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
