{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM, Input\n",
    "from keras.layers import Bidirectional\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "from ray.rllib.offline.json_reader import JsonReader\n",
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "import pandas as pd\n",
    "from true_state_viewer import TrueStateTreeGraphViz, display_tree_pairs\n",
    "from tqdm import trange\n",
    "import os\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.tune.registry  import register_env\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorldMovelEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(STATE_LEN,))\n",
    "        self.action_space = gym.spaces.Discrete(ACTION_LEN)\n",
    "        self.step_count = 0\n",
    "        \n",
    "        reward_to_index = np.load('reward_to_index.npy', allow_pickle=True).item()\n",
    "        self.number_rewards = int(len(reward_to_index.keys()))\n",
    "\n",
    "        #Reward Model\n",
    "        self.r_model = Sequential()\n",
    "        self.r_model.add(Input(shape=(STATE_LEN*2+1,)))\n",
    "        self.r_model.add(Dense(512, activation='relu'))\n",
    "        self.r_model.add(Dense(512, activation='relu'))\n",
    "        self.r_model.add(Dense(512, activation='relu'))\n",
    "        self.r_model.add(Dense(self.number_rewards, activation='softmax'))\n",
    "        self.r_model.load_weights('RewardModel_Insomnia')\n",
    "        \n",
    "        input_ = Input(shape=(STATE_LEN+ACTION_LEN+1,))\n",
    "        outs = []\n",
    "        for i in range(NUM_NODES):\n",
    "            for n in NODE_CLASSES:\n",
    "                x_ = Dense(128, activation='relu')(input_)\n",
    "                outs.append(Dense(n, activation='softmax', name=str(i)+str(n))(x_))\n",
    "\n",
    "        self.ns_model_multi_model = Model(input_, outs)\n",
    "        self.ns_model_multi_model.load_weights('AfterStateModel_Insomnia')\n",
    "     \n",
    "        self.reward_map = np.load('index_to_reward.npy', allow_pickle=True).item()\n",
    "        self.init_state = np.array([0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
    "         0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
    "         1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
    "         1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.])\n",
    "        self.state = self.init_state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "       \n",
    "        action_vec = np.zeros(ACTION_LEN)\n",
    "        action_vec[action] = 1\n",
    "        \n",
    "        state_action = np.concatenate([self.state, [self.step_count/100], action_vec], axis=-1)\n",
    "      \n",
    "        probs = self.ns_model_multi_model.predict(np.array([state_action]), verbose=0)\n",
    "        next_state = np.zeros(STATE_LEN)\n",
    "        index_state = 0; index = 0\n",
    "        for i in range(NUM_NODES):\n",
    "            for n in NODE_CLASSES:\n",
    "                next_state[index_state+np.random.choice(np.arange(n), p=probs[index][0])] = 1\n",
    "                index_state += n; index += 1\n",
    "        \n",
    "        reward_probs = self.r_model.predict(np.array([np.concatenate([self.state, next_state, [self.step_count/100]])]))\n",
    "        reward_index = np.random.choice(np.arange(self.number_rewards), p=reward_probs[0])\n",
    "        reward = self.reward_map[reward_index]\n",
    "        \n",
    "        self.state = next_state\n",
    "        \n",
    "        done = self.step_count == 99\n",
    "        if done:\n",
    "            self.step_count = 0\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        step_count = 0\n",
    "        self.state = self.init_state \n",
    "        return self.init_state\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "def dream_env_creator(config):\n",
    "    return WorldMovelEnv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(iteration):\n",
    "\n",
    "    input_reader = JsonReader(LOGS_PATH)\n",
    "    num_episodes = int((CAGE_Rollout_Batch_Size/100)*CAGE_Rollout_Iters)\n",
    "    num_data_points = num_episodes * 99\n",
    "    print(num_data_points)\n",
    "    states_t = np.ones((num_data_points, STATE_LEN+1))\n",
    "    actions_onehot = np.zeros((num_data_points, ACTION_LEN))\n",
    "    rewards = np.ones(num_data_points) \n",
    "    next_states = np.ones((num_data_points, STATE_LEN))\n",
    "    inital_states = []\n",
    "    \n",
    "    data = input_reader.next()\n",
    "    episodes_per_batch = int(data['obs'].shape[0]//100)\n",
    "\n",
    "    data_index = 0\n",
    "    for b in trange(int(num_episodes/episodes_per_batch)):\n",
    "        data = input_reader.next()\n",
    "        batch_index = 0\n",
    "        for e in range(episodes_per_batch):\n",
    "            states_t[data_index:data_index+99,:] = np.concatenate([data['obs'][batch_index:batch_index+99],np.arange(1,100).reshape(99,1)/100], axis=1)\n",
    "            next_states[data_index:data_index+99,:] = data['obs'][batch_index+1:batch_index+100]\n",
    "            rewards[data_index:data_index+99] = data['rewards'][batch_index:batch_index+99]\n",
    "            actions_onehot[np.arange(data_index,data_index+99),data['actions'][batch_index:batch_index+99]] = 1  \n",
    "            data_index += 99\n",
    "            batch_index += 100\n",
    "            if not data['obs'][0].tobytes() in inital_states:\n",
    "                inital_states.append(data['obs'][0].tobytes())\n",
    "                \n",
    "    for i in range(rewards.shape[0]):\n",
    "        if rewards[i] <-10:\n",
    "            rewards[i] == -11\n",
    "    \n",
    "    print(states_t.shape)\n",
    "    #Get data already processed\n",
    "    if os.path.isfile(LOGS_PATH + '/data/states_t.npy'):\n",
    "        states_t_, next_states_, actions_, rewards_ = load_data()\n",
    "        states_t = np.concatenate([states_t_, states_t])\n",
    "        actions_onehot = np.concatenate([actions_, actions_onehot])\n",
    "        rewards = np.concatenate([rewards_, rewards])\n",
    "        next_states = np.concatenate([next_states_, next_states])\n",
    "    else: \n",
    "        #Set up reward map\n",
    "        labels, encoding = np.unique(rewards, return_inverse=True)\n",
    "        index_to_reward = {}; reward_to_index = {}\n",
    "        for i in range(labels.shape[0]):\n",
    "            index_to_reward[i] = labels[i]\n",
    "            reward_to_index[labels[i]] = i\n",
    "        np.save('index_to_reward.npy', index_to_reward) \n",
    "        np.save('reward_to_index.npy', reward_to_index) \n",
    "    print(states_t.shape)\n",
    "    if not os.path.exists(LOGS_PATH + '/data'):\n",
    "        os.mkdir(LOGS_PATH + '/data')   \n",
    "    states_t = np.array(states_t, dtype=np.int8)\n",
    "    next_states = np.array(next_states, dtype=np.int8)\n",
    "    actions_onehot = np.array(actions_onehot, dtype=np.int8)\n",
    "    np.save(LOGS_PATH + '/data/states_t.npy', states_t)\n",
    "    np.save(LOGS_PATH + '/data/next_states.npy', next_states)\n",
    "    np.save(LOGS_PATH + '/data/rewards.npy', rewards)\n",
    "    np.save(LOGS_PATH + '/data/actions_onehot.npy', actions_onehot)\n",
    "\n",
    "    #Delete old logs\n",
    "    filelist = [ f for f in os.listdir('logs/APPO/Insomnia0') if f.endswith(\".json\") ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join('logs/APPO/Insomnia0', f))\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    states_t = np.load(LOGS_PATH + '/data/states_t.npy')\n",
    "    next_states = np.load(LOGS_PATH + '/data/next_states.npy')\n",
    "    actions = np.load(LOGS_PATH + '/data/actions_onehot.npy')\n",
    "    rewards = np.load(LOGS_PATH + '/data/rewards.npy')\n",
    "    return states_t, next_states, actions, rewards\n",
    "\n",
    "\n",
    "def train_state_tranistion_model():\n",
    "    states_t = np.load(LOGS_PATH + '/data/states_t.npy')\n",
    "    next_states = np.load(LOGS_PATH + '/data/next_states.npy')\n",
    "    actions = np.load(LOGS_PATH + '/data/actions_onehot.npy')\n",
    "    \n",
    "    states_actions_t = np.concatenate([states_t, actions], axis=1)\n",
    "    \n",
    "    NUM_NODES = 13\n",
    "    NODE_CLASSES = [3, 4]\n",
    "    data_map = {}\n",
    "    losses = []\n",
    "    index = 0\n",
    "    for i in range(NUM_NODES):\n",
    "        for n in NODE_CLASSES:\n",
    "            data_map[str(i)+str(n)] = next_states[:,index:index+n]\n",
    "            losses.append(tf.keras.losses.CategoricalCrossentropy())\n",
    "            index += n\n",
    "\n",
    "    input_ = Input(shape=(STATE_LEN+ACTION_LEN+1,))\n",
    "    x = Dense(256, activation='relu')(input_)\n",
    "    outs = []\n",
    "    for i in range(NUM_NODES):\n",
    "        for n in NODE_CLASSES:\n",
    "            x_ = Dense(128, activation='relu')(input_)\n",
    "            outs.append(Dense(n, activation='softmax', name=str(i)+str(n))(x_))\n",
    "\n",
    "    model = Model(input_, outs)\n",
    "    if os.path.isfile('AfterStateModel_Insomnia.index'):\n",
    "        model.load_weights('AfterStateModel_Insomnia')\n",
    "    model.compile(optimizer='adam', loss=losses, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.01)\n",
    "    K.set_value(model.optimizer.learning_rate, 0.001)\n",
    "    model.fit(states_actions_t, data_map, epochs=WM_Training_Iters, validation_split=0.15, verbose=0, callbacks=[callback], batch_size=256)\n",
    "    model.save_weights('AfterStateModel_Insomnia')\n",
    "    \n",
    "def train_reward_model():\n",
    "    rewards = np.load(LOGS_PATH + '/data/rewards.npy')\n",
    "    reward_to_index = np.load('reward_to_index.npy', allow_pickle=True).item()\n",
    "    reward_classes = np.vectorize(reward_to_index.get)(rewards)\n",
    "    reward_classes[np.isnan(reward_classes)] = -10 #Incase an unknown reward pops up\n",
    "    reward_onehot = np.eye(int(len(reward_to_index.keys())))[np.array(reward_classes, dtype=np.int8)]\n",
    "    \n",
    "    states_t = np.load(LOGS_PATH + '/data/states_t.npy')\n",
    "    next_states = np.load(LOGS_PATH + '/data/next_states.npy')\n",
    "    state_concate = np.concatenate([states_t, next_states], axis=1)\n",
    "     \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(STATE_LEN*2+1,)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(int(len(reward_to_index.keys())), activation='softmax'))\n",
    "    if os.path.isfile('RewardModel_Insomnia.index'):\n",
    "        model.load_weights('RewardModel_Insomnia')\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=3, min_delta=0.01)\n",
    "    K.set_value(model.optimizer.learning_rate, 0.001)\n",
    "    model.fit(state_concate, reward_onehot, epochs=WM_Training_Iters, validation_split=0.15, verbose=0, callbacks=[callback], batch_size=256)\n",
    "    model.save_weights('RewardModel_Insomnia')\n",
    "    \n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    reward.append(r_mean)\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    # import pdb; pdb.set_trace()\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2_No_Decoy.yaml'\n",
    "    if RED_AGENT == \"B_Line\":\n",
    "        agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    else:\n",
    "        agents = {\"Red\": RedMeanderAgent, \"Green\": GreenAgent}\n",
    "\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def rollout_in_CAGE(agent_checkpoint, iters):\n",
    "    register_env(name=\"CybORG\", env_creator=env_creator)\n",
    "    '''\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        #Each rollout worker uses a single cpu\n",
    "        .rollouts(num_rollout_workers=20, num_envs_per_worker=1, horizon=100)\\\n",
    "        .training(sgd_minibatch_size = 100, train_batch_size=CAGE_Rollout_Batch_Size, gamma=0.99, lr=0.00001, \n",
    "                  model={\"fcnet_hiddens\": [256, 256], \"fcnet_activation\": \"tanh\",})\\\n",
    "        .environment(disable_env_checking=True, env = 'CybORG')\\\n",
    "        .resources(num_gpus=1)\\\n",
    "        .framework('torch')\\\n",
    "        .offline_data(output=LOGS_PATH, output_compress_columns=['prev_actions', 'prev_rewards', 'dones', 't', 'action_prob', 'action_logp', 'action_dist_inputs', 'advantages', 'value_targets'], #'eps_id', 'unroll_id', 'agent_index',\n",
    "                 output_config={\"format\": \"json\"},)\n",
    "        #.exploration(explore=True, exploration_config={\"type\": \"RE3\", \"embeds_dim\": 128, \"beta_schedule\": \"constant\", \"sub_exploration\": {\"type\": \"StochasticSampling\",},})\\\n",
    "    )\n",
    "    '''\n",
    "    \n",
    "    config = (\n",
    "        DQNConfig()\n",
    "        #Each rollout worker uses a single cpu\n",
    "        .rollouts(num_rollout_workers=10, num_envs_per_worker=1, horizon=100)\\\n",
    "        .training(num_atoms=21, v_min=-50, v_max=0, noisy=True, dueling=True,\n",
    "                  double_q=True, gamma=0.99, lr=0.00005, n_step=5,\n",
    "                  train_batch_size=2000,\n",
    "                  model={\"fcnet_hiddens\": [256, 256], \"fcnet_activation\": \"tanh\",})\\\n",
    "        .environment(disable_env_checking=True, env = 'CybORG')\\\n",
    "        .resources(num_gpus=1)\\\n",
    "        .framework('torch')\\\n",
    "        .offline_data(output=LOGS_PATH, output_compress_columns=['prev_actions', 'prev_rewards', 'dones', 't', 'action_prob', 'action_logp', 'action_dist_inputs', 'advantages', 'value_targets'], #'eps_id', 'unroll_id', 'agent_index',\n",
    "                 output_config={\"format\": \"json\"},)\n",
    "        #.exploration(explore=True, exploration_config={\"type\": \"RE3\", \"embeds_dim\": 128, \"beta_schedule\": \"constant\", \"sub_exploration\": {\"type\": \"StochasticSampling\",},})\\\n",
    "    )\n",
    "    \n",
    "    trainer = config.build()\n",
    "    \n",
    "    if not agent_checkpoint == '':\n",
    "        trainer.load_checkpoint(agent_checkpoint)\n",
    "\n",
    "    for i in range(iters):\n",
    "        print_results(trainer.train())\n",
    "    agent_checkpoint = trainer.save(LOGS_PATH)\n",
    "    return agent_checkpoint\n",
    "    \n",
    "def dream(agent_checkpoint):\n",
    "    register_env(name=\"DreamCybORG\", env_creator=dream_env_creator)\n",
    "    '''\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        #Each rollout worker uses a single cpu\n",
    "        .rollouts(num_rollout_workers=20, num_envs_per_worker=1, horizon=100)\\\n",
    "        .training(sgd_minibatch_size = 100, train_batch_size=2000, gamma=0.99, lr=0.00001, \n",
    "                  model={\"fcnet_hiddens\": [256, 256], \"fcnet_activation\": \"tanh\",})\\\n",
    "        .environment(disable_env_checking=True, env = 'CybORG')\\\n",
    "        .resources(num_gpus=1)\\\n",
    "        .framework('torch')\\\n",
    "        #.exploration(explore=True, exploration_config={\"type\": \"RE3\", \"embeds_dim\": 128, \"beta_schedule\": \"constant\", \"sub_exploration\": {\"type\": \"StochasticSampling\",},})\\\n",
    "    )\n",
    "    '''\n",
    "    config = (\n",
    "        DQNConfig()\n",
    "        #Each rollout worker uses a single cpu\n",
    "        .rollouts(num_rollout_workers=10, num_envs_per_worker=1, horizon=100)\\\n",
    "        .training(num_atoms=21, v_min=-50, v_max=0, noisy=True, dueling=True,\n",
    "                  double_q=True, gamma=0.99, lr=0.00005, n_step=5,\n",
    "                  train_batch_size=2000,\n",
    "                  model={\"fcnet_hiddens\": [256, 256], \"fcnet_activation\": \"tanh\",})\\\n",
    "        .environment(disable_env_checking=True, env = 'CybORG')\\\n",
    "        .resources(num_gpus=1)\\\n",
    "        .framework('torch')\\\n",
    "        .offline_data(output=LOGS_PATH, output_compress_columns=['prev_actions', 'prev_rewards', 'dones', 't', 'action_prob', 'action_logp', 'action_dist_inputs', 'advantages', 'value_targets'], #'eps_id', 'unroll_id', 'agent_index',\n",
    "                 output_config={\"format\": \"json\"},)\n",
    "        #.exploration(explore=True, exploration_config={\"type\": \"RE3\", \"embeds_dim\": 128, \"beta_schedule\": \"constant\", \"sub_exploration\": {\"type\": \"StochasticSampling\",},})\\\n",
    "    )\n",
    "    trainer = config.build()\n",
    "    trainer.load_checkpoint(agent_checkpoint)\n",
    "    for i in range(Learning_In_Dream_Iters):\n",
    "        print_results(trainer.train())\n",
    "    agent_checkpoint = trainer.save(LOGS_PATH)\n",
    "    return agent_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLLOUT:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 14:43:41,268\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=9467)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9467)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9470)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9470)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9472)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9472)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9476)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9476)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9479)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9479)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9466)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9466)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9468)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9468)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9482)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9482)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9477)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9477)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "\u001b[2m\u001b[36m(pid=9473)\u001b[0m /usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(pid=9473)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
      "2023-02-19 14:44:00,729\tINFO trainable.py:172 -- Trainable.setup took 22.901 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-02-19 14:44:01,202\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 \tr_mean: -422.2 \tr_max: -194.4 \tr_min: -878.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 14:44:08,530\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2 \tr_mean: -468.5 \tr_max: -192.2 \tr_min: -1078.1\n",
      "   3 \tr_mean: -488.0 \tr_max: -153.5 \tr_min: -1078.1\n",
      "   4 \tr_mean: -456.1 \tr_max: -142.7 \tr_min: -1078.1\n",
      "   5 \tr_mean: -463.9 \tr_max: -142.7 \tr_min: -1078.1\n",
      "   6 \tr_mean: -480.6 \tr_max: -142.7 \tr_min: -1078.1\n",
      "   7 \tr_mean: -474.1 \tr_max: -142.7 \tr_min: -1078.1\n",
      "   8 \tr_mean: -464.1 \tr_max: -142.7 \tr_min: -1078.1\n",
      "   9 \tr_mean: -465.1 \tr_max: -142.7 \tr_min: -1078.1\n",
      "  10 \tr_mean: -466.4 \tr_max: -142.7 \tr_min: -1136.7\n",
      "  11 \tr_mean: -451.2 \tr_max: -142.7 \tr_min: -1136.7\n",
      "  12 \tr_mean: -443.4 \tr_max: -142.7 \tr_min: -1136.7\n",
      "  13 \tr_mean: -418.8 \tr_max: -142.7 \tr_min: -1136.7\n",
      "  14 \tr_mean: -429.0 \tr_max: -148.6 \tr_min: -1136.7\n",
      "  15 \tr_mean: -421.5 \tr_max: -148.6 \tr_min: -1136.7\n",
      "  16 \tr_mean: -405.8 \tr_max: -145.7 \tr_min: -1136.7\n",
      "  17 \tr_mean: -407.5 \tr_max: -145.7 \tr_min: -1136.7\n",
      "  18 \tr_mean: -400.2 \tr_max: -145.7 \tr_min: -1136.7\n",
      "  19 \tr_mean: -386.9 \tr_max: -141.9 \tr_min: -1136.7\n",
      "  20 \tr_mean: -369.5 \tr_max: -141.9 \tr_min: -1079.2\n",
      "  21 \tr_mean: -369.1 \tr_max: -141.9 \tr_min: -1079.2\n",
      "  22 \tr_mean: -355.0 \tr_max: -141.9 \tr_min: -1079.2\n",
      "  23 \tr_mean: -357.6 \tr_max: -141.9 \tr_min: -1079.2\n",
      "  24 \tr_mean: -350.9 \tr_max: -141.8 \tr_min: -1079.2\n",
      "  25 \tr_mean: -347.9 \tr_max: -141.8 \tr_min: -1079.2\n",
      "  26 \tr_mean: -330.4 \tr_max: -141.8 \tr_min: -1061.9\n",
      "  27 \tr_mean: -322.2 \tr_max: -136.0 \tr_min: -1061.9\n",
      "  28 \tr_mean: -314.2 \tr_max: -136.0 \tr_min: -1061.9\n",
      "  29 \tr_mean: -321.6 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  30 \tr_mean: -315.5 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  31 \tr_mean: -328.9 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  32 \tr_mean: -341.9 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  33 \tr_mean: -355.6 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  34 \tr_mean: -347.7 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  35 \tr_mean: -347.4 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  36 \tr_mean: -349.0 \tr_max: -136.0 \tr_min: -1145.7\n",
      "  37 \tr_mean: -355.9 \tr_max: -140.1 \tr_min: -1145.7\n",
      "  38 \tr_mean: -375.2 \tr_max: -140.1 \tr_min: -1145.7\n",
      "  39 \tr_mean: -364.7 \tr_max: -140.1 \tr_min: -1091.4\n",
      "  40 \tr_mean: -372.4 \tr_max: -140.1 \tr_min: -1091.4\n",
      "  41 \tr_mean: -365.5 \tr_max: -141.0 \tr_min: -1091.4\n",
      "  42 \tr_mean: -345.6 \tr_max: -141.0 \tr_min: -1091.4\n",
      "  43 \tr_mean: -334.4 \tr_max: -141.0 \tr_min: -1082.7\n",
      "  44 \tr_mean: -333.3 \tr_max: -141.0 \tr_min: -1082.7\n",
      "  45 \tr_mean: -320.7 \tr_max: -141.0 \tr_min: -1082.7\n",
      "  46 \tr_mean: -336.3 \tr_max: -141.0 \tr_min: -1082.7\n",
      "  47 \tr_mean: -327.4 \tr_max: -146.3 \tr_min: -1082.7\n",
      "  48 \tr_mean: -317.4 \tr_max: -146.3 \tr_min: -856.7\n",
      "  49 \tr_mean: -307.6 \tr_max: -138.7 \tr_min: -856.7\n",
      "  50 \tr_mean: -315.6 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  51 \tr_mean: -329.8 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  52 \tr_mean: -344.8 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  53 \tr_mean: -338.3 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  54 \tr_mean: -329.7 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  55 \tr_mean: -335.4 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  56 \tr_mean: -330.6 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  57 \tr_mean: -316.8 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  58 \tr_mean: -317.1 \tr_max: -138.7 \tr_min: -1085.3\n",
      "  59 \tr_mean: -316.7 \tr_max: -153.7 \tr_min: -1085.3\n",
      "  60 \tr_mean: -301.9 \tr_max: -153.7 \tr_min: -1042.6\n",
      "  61 \tr_mean: -289.6 \tr_max: -142.9 \tr_min: -1032.0\n",
      "  62 \tr_mean: -274.4 \tr_max: -142.9 \tr_min: -905.2\n",
      "  63 \tr_mean: -271.8 \tr_max: -142.9 \tr_min: -905.2\n",
      "  64 \tr_mean: -273.6 \tr_max: -142.9 \tr_min: -905.2\n",
      "  65 \tr_mean: -263.7 \tr_max: -142.9 \tr_min: -905.2\n",
      "  66 \tr_mean: -259.8 \tr_max: -142.9 \tr_min: -905.2\n",
      "  67 \tr_mean: -267.9 \tr_max: -142.9 \tr_min: -905.2\n",
      "  68 \tr_mean: -257.1 \tr_max: -142.9 \tr_min: -713.0\n",
      "  69 \tr_mean: -260.1 \tr_max: -142.9 \tr_min: -713.0\n",
      "  70 \tr_mean: -258.1 \tr_max: -142.9 \tr_min: -713.0\n",
      "  71 \tr_mean: -255.4 \tr_max: -162.9 \tr_min: -713.0\n",
      "  72 \tr_mean: -269.2 \tr_max: -162.9 \tr_min: -809.5\n",
      "  73 \tr_mean: -275.0 \tr_max: -162.9 \tr_min: -809.5\n",
      "  74 \tr_mean: -276.3 \tr_max: -162.9 \tr_min: -809.5\n",
      "  75 \tr_mean: -294.1 \tr_max: -162.9 \tr_min: -916.0\n",
      "  76 \tr_mean: -312.1 \tr_max: -162.9 \tr_min: -1079.9\n",
      "  77 \tr_mean: -311.2 \tr_max: -162.9 \tr_min: -1079.9\n",
      "  78 \tr_mean: -320.3 \tr_max: -162.9 \tr_min: -1079.9\n",
      "  79 \tr_mean: -324.9 \tr_max: -162.9 \tr_min: -1079.9\n",
      "  80 \tr_mean: -338.7 \tr_max: -162.9 \tr_min: -1079.9\n",
      "  81 \tr_mean: -353.9 \tr_max: -163.1 \tr_min: -1079.9\n",
      "  82 \tr_mean: -356.6 \tr_max: -154.1 \tr_min: -1079.9\n",
      "  83 \tr_mean: -377.9 \tr_max: -154.1 \tr_min: -1079.9\n",
      "  84 \tr_mean: -403.5 \tr_max: -154.1 \tr_min: -1079.9\n",
      "  85 \tr_mean: -399.8 \tr_max: -154.1 \tr_min: -1079.9\n",
      "  86 \tr_mean: -408.6 \tr_max: -139.9 \tr_min: -1062.8\n",
      "  87 \tr_mean: -432.4 \tr_max: -139.9 \tr_min: -1070.8\n",
      "  88 \tr_mean: -436.0 \tr_max: -139.9 \tr_min: -1070.8\n",
      "  89 \tr_mean: -480.8 \tr_max: -139.9 \tr_min: -1091.6\n",
      "  90 \tr_mean: -496.7 \tr_max: -139.9 \tr_min: -1091.6\n",
      "  91 \tr_mean: -496.7 \tr_max: -139.9 \tr_min: -1091.6\n",
      "  92 \tr_mean: -513.1 \tr_max: -139.9 \tr_min: -1091.6\n",
      "  93 \tr_mean: -530.2 \tr_max: -139.9 \tr_min: -1093.4\n",
      "  94 \tr_mean: -534.3 \tr_max: -122.8 \tr_min: -1093.4\n",
      "  95 \tr_mean: -555.9 \tr_max: -122.8 \tr_min: -1093.4\n",
      "  96 \tr_mean: -555.2 \tr_max: -122.8 \tr_min: -1093.4\n",
      "  97 \tr_mean: -554.5 \tr_max: -122.8 \tr_min: -1093.4\n",
      "  98 \tr_mean: -583.7 \tr_max: -122.8 \tr_min: -1093.4\n",
      "  99 \tr_mean: -570.8 \tr_max: -122.8 \tr_min: -1123.7\n",
      " 100 \tr_mean: -591.2 \tr_max: -122.8 \tr_min: -1138.7\n",
      " 101 \tr_mean: -614.4 \tr_max: -122.8 \tr_min: -1138.7\n",
      " 102 \tr_mean: -613.2 \tr_max: -122.8 \tr_min: -1138.7\n",
      " 103 \tr_mean: -606.0 \tr_max: -122.8 \tr_min: -1138.7\n",
      " 104 \tr_mean: -609.5 \tr_max: -145.4 \tr_min: -1138.7\n",
      " 105 \tr_mean: -626.1 \tr_max: -145.4 \tr_min: -1138.7\n",
      " 106 \tr_mean: -610.4 \tr_max: -143.2 \tr_min: -1138.7\n",
      " 107 \tr_mean: -631.3 \tr_max: -143.2 \tr_min: -1138.7\n",
      " 108 \tr_mean: -643.2 \tr_max: -143.2 \tr_min: -1138.7\n",
      " 109 \tr_mean: -657.4 \tr_max: -143.2 \tr_min: -1138.7\n",
      " 110 \tr_mean: -634.3 \tr_max: -143.2 \tr_min: -1108.3\n",
      " 111 \tr_mean: -624.7 \tr_max: -143.2 \tr_min: -1108.3\n",
      " 112 \tr_mean: -625.3 \tr_max: -143.2 \tr_min: -1108.3\n",
      " 113 \tr_mean: -634.5 \tr_max: -143.2 \tr_min: -1108.3\n",
      " 114 \tr_mean: -626.4 \tr_max: -143.2 \tr_min: -1108.3\n",
      " 115 \tr_mean: -594.0 \tr_max: -143.2 \tr_min: -1094.8\n",
      " 116 \tr_mean: -594.8 \tr_max: -205.9 \tr_min: -1098.3\n",
      " 117 \tr_mean: -568.2 \tr_max: -205.9 \tr_min: -1098.3\n",
      " 118 \tr_mean: -530.8 \tr_max: -205.9 \tr_min: -1098.3\n",
      " 119 \tr_mean: -488.2 \tr_max: -171.7 \tr_min: -1098.3\n",
      " 120 \tr_mean: -473.4 \tr_max: -171.7 \tr_min: -1098.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LOGS_PATH = 'logs/APPO/InsomniaDQN'\n",
    "CAGE_Rollout_Iters_Inital = 200 \n",
    "CAGE_Rollout_Iters_After = 20\n",
    "CAGE_Rollout_Batch_Size = 2000\n",
    "WM_Training_Iters = 25\n",
    "Learning_In_Dream_Iters = 20\n",
    "STATE_LEN = 91\n",
    "ACTION_LEN = 41\n",
    "RED_AGENT = 'B_Line' \n",
    "NUM_NODES = 13\n",
    "NODE_CLASSES = [3, 4]\n",
    "reward = [] #np.load('insomnia_r.npy').tolist()\n",
    "\n",
    "agent_checkpoint = ''\n",
    "CAGE_Rollout_Iters = CAGE_Rollout_Iters_Inital\n",
    "\n",
    "for i in range(1,15):\n",
    "    print(\"ROLLOUT: \", i)\n",
    "    if i == 1:\n",
    "        agent_checkpoint = rollout_in_CAGE(agent_checkpoint, CAGE_Rollout_Iters_Inital)\n",
    "    else: \n",
    "        CAGE_Rollout_Iters = CAGE_Rollout_Iters_After\n",
    "        agent_checkpoint = rollout_in_CAGE(agent_checkpoint, CAGE_Rollout_Iters)\n",
    "    print(\"PROCESS DATA: \", i)\n",
    "    process_data(i)\n",
    "    print(\"TRAIN STATE TRANISION MODEL: \", i)\n",
    "    train_state_tranistion_model()\n",
    "    K.clear_session()\n",
    "    print(\"TRAIN REWARD MODEL: \", i)\n",
    "    train_reward_model()\n",
    "    K.clear_session()\n",
    "    print(\"DREAM: \", i)\n",
    "    agent_checkpoint = dream(agent_checkpoint)\n",
    "    np.save('insomnia_r.npy', np.array(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "interval = 20\n",
    "fig = plt.figure(figsize=(12, 8), dpi=100)\n",
    "plt.title('PPO Training With World Model - CAGEv2 No Decoy')\n",
    "x = 0\n",
    "plt.plot(np.arange(x,x+100), reward[x:x+100], linestyle='solid', color='black', label='Learning In CAGE')\n",
    "x = interval\n",
    "plt.plot(np.arange(x+interval,x+(interval*2)), reward[x+interval:x+(interval*2)], linestyle='dotted', color='black', label='Learning In The Dream')\n",
    "for i in range(1,6):\n",
    "    x = interval * (i*2)\n",
    "    plt.plot(np.arange(x,x+interval), reward[x:x+interval], linestyle='solid', color='black')\n",
    "    plt.plot(np.arange(x+interval,x+(interval*2)), reward[x+interval:x+(interval*2)], linestyle='dotted', color='black')\n",
    "plt.ylabel('Mean Reward On Batch')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward[x+interval:x+(interval*2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.save('insomnia_r.npy', np.array(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11,31):\n",
    "    print(\"ROLLOUT: \", i)\n",
    "    if i == 1:\n",
    "        agent_checkpoint = rollout_in_CAGE(agent_checkpoint, CAGE_Rollout_Iters_Inital)\n",
    "    else: \n",
    "        CAGE_Rollout_Iters = CAGE_Rollout_Iters_After\n",
    "        agent_checkpoint = rollout_in_CAGE(agent_checkpoint, CAGE_Rollout_Iters)\n",
    "    print(\"PROCESS DATA: \", i)\n",
    "    process_data(i)\n",
    "    print(\"TRAIN STATE TRANISION MODEL: \", i)\n",
    "    train_state_tranistion_model()\n",
    "    print(\"TRAIN REWARD MODEL: \", i)\n",
    "    train_reward_model()\n",
    "    print(\"DREAM: \", i)\n",
    "    agent_checkpoint = dream(agent_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_t = np.load(LOGS_PATH + '/data/states_t.npy')\n",
    "states_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1052 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/ray_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
