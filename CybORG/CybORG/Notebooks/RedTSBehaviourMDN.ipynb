{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDN References:\n",
    "\n",
    "[useful pytorch reference](https://github.com/tonyduan/mixture-density-network)\n",
    "\n",
    "[keras version](https://github.com/cpmpercussion/keras-mdn-layer)\n",
    "\n",
    "[another keras version](https://github.com/omimo/Keras-MDN/blob/master/kmdn/mdn.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MDN\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import pandas as pd\n",
    "from ProcessTrueStateActionData import read_df_in_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data \n",
    "(produce tf train+test datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_test_split = 0.8\n",
    "\n",
    "LATENT_SIZE = 8 # (mdn output_dimension)\n",
    "NUMBER_MIXTURES = 5\n",
    "\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_for_training():\n",
    "    true_states = [\"pre\",\"blue\",\"red\"]\n",
    "    ts_columns = {}\n",
    "    for true_state in true_states:\n",
    "        ts_columns[true_state] = []\n",
    "        for node in range(13):\n",
    "            ts_columns[true_state].append(f\"{node}_ts_{true_state}_known_status\")\n",
    "            ts_columns[true_state].append(f\"{node}_ts_{true_state}_access_status\")\n",
    "    return ts_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_dict = get_columns_for_training()\n",
    "pre_cols, blue_cols, red_cols = cols_dict[\"pre\"], cols_dict[\"blue\"], cols_dict[\"red\"]\n",
    "all_cols = pre_cols + blue_cols + red_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet(\"csv_data/TrueStatesObsActsRwds_1221_4000_B_Line.parquet\")\n",
    "df = df[all_cols].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                      39072000\n",
       "0_ts_pre_known_status       4884124\n",
       "0_ts_pre_access_status      4884116\n",
       "1_ts_pre_known_status       4884132\n",
       "1_ts_pre_access_status      4884132\n",
       "                             ...   \n",
       "10_ts_red_access_status     4884132\n",
       "11_ts_red_known_status      4884124\n",
       "11_ts_red_access_status     4884132\n",
       "12_ts_red_known_status      4884124\n",
       "12_ts_red_access_status     4884132\n",
       "Length: 79, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "train_df=df.sample(frac=train_test_split,random_state=42)\n",
    "train_pre_df = train_df[pre_cols]\n",
    "train_blue_df = train_df[blue_cols]\n",
    "train_red_df = train_df[red_cols]\n",
    "\n",
    "test_df=df.drop(train_df.index)\n",
    "test_pre_df = test_df[pre_cols]\n",
    "test_blue_df = test_df[blue_cols]\n",
    "test_red_df = test_df[red_cols]\n",
    "\n",
    "train_size = train_df.shape[0]\n",
    "test_size = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3907200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices(((train_pre_df.values,train_blue_df.values),train_red_df.values)).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(((test_pre_df.values,test_blue_df.values),test_red_df.values)).shuffle(test_size).batch(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(64, 26), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 1, 0],\n",
      "       [0, 0, 2, ..., 0, 2, 2],\n",
      "       [0, 0, 0, ..., 0, 1, 0],\n",
      "       ...,\n",
      "       [0, 0, 2, ..., 2, 1, 0],\n",
      "       [0, 0, 2, ..., 2, 1, 0],\n",
      "       [1, 0, 2, ..., 0, 2, 2]])>, <tf.Tensor: shape=(64, 26), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 1, 0],\n",
      "       [0, 0, 2, ..., 0, 2, 2],\n",
      "       [0, 0, 0, ..., 0, 1, 0],\n",
      "       ...,\n",
      "       [0, 0, 2, ..., 2, 1, 0],\n",
      "       [0, 0, 2, ..., 2, 1, 0],\n",
      "       [1, 0, 2, ..., 0, 2, 2]])>), <tf.Tensor: shape=(64, 26), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 1, 0],\n",
      "       [0, 0, 2, ..., 0, 2, 2],\n",
      "       [0, 0, 0, ..., 0, 1, 0],\n",
      "       ...,\n",
      "       [0, 0, 2, ..., 2, 1, 0],\n",
      "       [0, 0, 2, ..., 2, 1, 0],\n",
      "       [1, 0, 2, ..., 0, 2, 2]])>)\n"
     ]
    }
   ],
   "source": [
    "for row in train_dataset.take(1):\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an MDN based model with pretrained encoder/decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedTSPrediction(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vae_path, latent_size, num_mixtures):\n",
    "        super().__init__()\n",
    "        self.ts_vae = tf.keras.models.load_model(vae_path)\n",
    "        self.ts_vae.trainable = False\n",
    "        self.encoder = self.ts_vae.encoder\n",
    "        self.decoder = self.ts_vae.decoder\n",
    "\n",
    "        self.ts_dense = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.cross_dense = tf.keras.layers.Dense(2048, activation=tf.nn.relu)\n",
    "\n",
    "        self.mdn = MDN.MDN(output_dimension=latent_size, num_mixtures=num_mixtures)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mean, logvar = self.ts_vae.encode(x)\n",
    "        z = self.ts_vae.reparameterize(mean, logvar)\n",
    "        return z\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        pre_ts = inputs[0]\n",
    "        blue_ts = inputs[1]\n",
    "        \n",
    "        pre_ts_oh = tf.reshape(tf.one_hot(pre_ts,3),(-1,78))\n",
    "#         pre_ts_access = tf.reshape(tf.one_hot(pre_ts[:,13:],3),(-1,39))\n",
    "        \n",
    "        blue_ts_oh = tf.reshape(tf.one_hot(blue_ts,3),(-1,78))\n",
    "#         blue_ts_access = tf.reshape(tf.one_hot(blue_ts[:,13:],3),(-1,39))\n",
    "        \n",
    "#         print(pre_ts[:,:])\n",
    "#         print(pre_ts_oh)\n",
    "#         print(blue_ts)\n",
    "#         blue_ts_kn = K.print_tensor(blue_ts[:,:13], message='blue known = ')\n",
    "#         blue_ts_known = K.print_tensor(blue_ts_known[:,:13], message='blue known OH = ')\n",
    "#         print(blue_ts_known.shape)\n",
    "\n",
    "        pre_ts_encoded = self.ts_dense(self.encode(pre_ts_oh))\n",
    "        blue_ts_encoded = self.ts_dense(self.encode(blue_ts_oh))\n",
    "\n",
    "        combined = tf.keras.layers.concatenate([pre_ts_encoded, blue_ts_encoded])\n",
    "\n",
    "        combined_hidden = self.cross_dense(combined)\n",
    "\n",
    "        mdn_out = self.mdn(combined_hidden)\n",
    "\n",
    "        return mdn_out\n",
    "\n",
    "    def decode(self, latent_pred):\n",
    "        return self.decoder(latent_pred)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mixture_loss_func(output_dim, num_mixes, model_encode):\n",
    "#     \"\"\"Construct a loss functions for the MDN layer parametrised by number of mixtures.\"\"\"\n",
    "#     # Construct a loss function with the right number of mixtures and outputs\n",
    "#     def mdn_loss_func(y_true, y_pred):\n",
    "        \n",
    "#         encoded_y_true = model_encode(y_true)\n",
    "        \n",
    "#         # Reshape inputs in case this is used in a TimeDistribued layer\n",
    "#         y_pred = tf.reshape(y_pred, [-1, (2 * num_mixes * output_dim) + num_mixes], name='reshape_ypreds')\n",
    "#         y_true = tf.reshape(encoded_y_true, [-1, output_dim], name='reshape_ytrue')\n",
    "#         # Split the inputs into paramaters\n",
    "#         out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "#                                                                          num_mixes * output_dim,\n",
    "#                                                                          num_mixes],\n",
    "#                                              axis=-1, name='mdn_coef_split')\n",
    "#         # Construct the mixture models\n",
    "#         cat = tfd.Categorical(logits=out_pi)\n",
    "#         component_splits = [output_dim] * num_mixes\n",
    "#         mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "#         sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "#         coll = [tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "#                 in zip(mus, sigs)]\n",
    "#         mixture = tfd.Mixture(cat=cat, components=coll)\n",
    "#         loss = mixture.log_prob(y_true)\n",
    "#         loss = tf.negative(loss)\n",
    "#         loss = tf.reduce_mean(loss)\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "red_ts_predictor = RedTSPrediction('models/trueStateVAE_7_L8', latent_size=LATENT_SIZE, num_mixtures=NUMBER_MIXTURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:281: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04560362  0.1059904   0.0295794  -0.08081593  0.13998106 -0.20732507\n",
      "  -0.07886654 -0.06673503 -0.07052223 -0.03862421  0.32742622  0.00316813\n",
      "   0.1123829   0.00564317 -0.07104158  0.1187254   0.22052075 -0.13135926\n",
      "  -0.28310847  0.12735707  0.0231918   0.10797485 -0.08647911  0.01596696\n",
      "  -0.02562372  0.03188619  0.00454077  0.14835453  0.18995728  0.07866741\n",
      "   0.24032064  0.21614648 -0.0921552  -0.27748036 -0.129478   -0.16879967\n",
      "  -0.05060436 -0.04307874  0.1046778   0.23691055  0.883837    0.9813433\n",
      "   1.0224891   1.0195285   1.2468647   1.159755    0.90905154  0.95568764\n",
      "   0.79917455  0.8934724   0.88116556  0.7583916   1.070764    1.0735124\n",
      "   0.91242427  1.3468823   0.9979929   0.90690356  1.1202555   0.9944802\n",
      "   0.93671894  0.9106967   0.89256805  1.1723987   1.1330614   0.9654097\n",
      "   1.0642014   0.9894095   0.94632083  1.2603776   1.2378305   0.9284266\n",
      "   1.0535481   1.1399404   1.1566311   0.9621086   0.90264946  1.1852226\n",
      "   1.1606067   1.2178311  -0.19309613  0.06719071 -0.01855429 -0.10557792\n",
      "  -0.12226932]], shape=(1, 85), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for row in test_dataset.take(1):\n",
    "#   print(row)\n",
    "  out = red_ts_predictor(row[0])\n",
    "  print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(model, x, y, loss_func):\n",
    "    out = model(x)\n",
    "    \n",
    "    y_oh = tf.reshape(tf.one_hot(y,3),(-1,78))\n",
    "    y_encoded = model.encode(y_oh)\n",
    "    \n",
    "    loss = loss_func(y_encoded, out)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y, optimizer, loss_func):\n",
    "    \"\"\"Executes one training step and returns the loss.\n",
    "\n",
    "    This function computes the loss and gradients, and uses the latter to\n",
    "    update the model's parameters.\n",
    "    \"\"\"\n",
    "#     y_encoded = model.encode(y)\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x, y, loss_func)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"red_ts_prediction\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " true_state_vae_4 (TrueState  multiple                 121736094 \n",
      " VAE)                                                            \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   (None, 16)                60892016  \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 78)                60844078  \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1152      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  526336    \n",
      "                                                                 \n",
      " mdn (MDN)                   multiple                  174165    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,437,747\n",
      "Trainable params: 701,653\n",
      "Non-trainable params: 121,736,094\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# red_ts_predictor.compile(loss=get_mixture_loss_func(LATENT_SIZE,NUMBER_MIXTURES,red_ts_predictor.encode), optimizer=tf.keras.optimizers.Adam(),metrics=['mean_squared_error'])\n",
    "# red_ts_predictor.build(((1,78),(1,78)))\n",
    "red_ts_predictor.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = MDN.get_mixture_loss_func(LATENT_SIZE,NUMBER_MIXTURES)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Test set loss: 4.044805526733398, time elapse for current epoch: 1.1332051753997803\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    for train_x, train_y in train_dataset.take(10):\n",
    "        if (count %1000) == 0:\n",
    "            print(f\"{count}={count*batch_size} samples\")\n",
    "        train_step(red_ts_predictor, train_x, train_y, optimizer, loss_func)\n",
    "        count += 1\n",
    "    end_time = time.time()\n",
    "\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x, test_y in train_dataset.take(10):#test_dataset:\n",
    "        loss(compute_loss(red_ts_predictor, test_x, test_y, loss_func))\n",
    "    loss = loss.result()\n",
    "    display.clear_output(wait=False)\n",
    "    print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'\n",
    "        .format(epoch, loss, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1025, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 989, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mred_ts_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileqnquvk12.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1025, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 989, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: No loss found. You may have forgotten to provide a `loss` argument in the `compile()` method.\n"
     ]
    }
   ],
   "source": [
    "# history = red_ts_predictor.fit(train_dataset, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
