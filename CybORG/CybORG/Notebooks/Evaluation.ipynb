{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import warnings\n",
    "import ray\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=378)\u001b[0m 2022-12-06 09:56:35,405\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-12-06 09:56:39,345\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n",
      "100%|██████████| 100/100 [01:32<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for red agent B_lineAgent and steps 100 is: -24.5 with a standard deviation of 16.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_EPS = 100\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return RLlibWrapper(agent_name=\"Blue\", env=env)\n",
    "\n",
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "register_env(name=\"CybORG\", env_creator=env_creator)\n",
    "\n",
    "config = PPOConfig().resources(num_gpus=1).environment(env = 'smac').rollouts(num_rollout_workers=1)\n",
    "\n",
    "trainer = config.build(\"CybORG\")\n",
    "path = 'results/APPO/explore/PPO_CybORG_5000a_00000_0_exploration_config=type_RE3_embeds_dim_128_beta_schedule_constant_sub_exploration_type_StochasticSampling_2022-12-05_22-01-04/checkpoint_004902'\n",
    "path = 'results/APPO/explore/PPO_CybORG_5000a_00001_1_exploration_config=type_StochasticSampling,fcnet_activation=tanh,fcnet_hiddens=256_256_2022-12-06_02-59-07/checkpoint_004902'\n",
    "trainer.load_checkpoint(path)\n",
    "\n",
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "obs = []\n",
    "#print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "for red_agent in [B_lineAgent]:#, RedMeanderAgent]:\n",
    "\n",
    "    cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "    wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "    observation = wrapped_cyborg.reset()\n",
    "    obs.append(observation)\n",
    "    # observation = cyborg.reset().observation\n",
    "\n",
    "    action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "    # action_space = cyborg.get_action_space(agent_name)\n",
    "    total_reward = []\n",
    "    actions = []\n",
    "    for i in trange(MAX_EPS):\n",
    "        r = []\n",
    "        #a = []\n",
    "        # cyborg.env.env.tracker.render()\n",
    "        for j in range(100):\n",
    "            action = trainer.compute_single_action(observation, explore=False)\n",
    "            #action_vec = np.zeros(145)\n",
    "            #action_vec[int(action)] = 1\n",
    "            #action = agent.get_action(observation, action_space)\n",
    "            observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "            obs.append(observation)\n",
    "            #actions.append(action_vec)\n",
    "            # result = cyborg.step(agent_name, action)\n",
    "            r.append(rew)\n",
    "            # r.append(result.reward)\n",
    "           # a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "        total_reward.append(sum(r))\n",
    "        # observation = cyborg.reset().observation\n",
    "        observation = wrapped_cyborg.reset()\n",
    "    print(f'Average reward for red agent {red_agent.__name__} and steps {100} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n",
    "    #return mean(total_reward), np.mean(np.array(obs), axis=0),  np.mean(np.array(actions), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
