{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.BaseAgent import BaseAgent\n",
    "from CybORG.Agents.SimpleAgents.BlueReactAgent import BlueReactRemoveAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers.EnumActionWrapper import EnumActionWrapper\n",
    "from CybORG.Agents.Wrappers.FixedFlatWrapper import FixedFlatWrapper\n",
    "from CybORG.Agents.Wrappers.OpenAIGymWrapper import OpenAIGymWrapper\n",
    "from CybORG.Agents.Wrappers.ReduceActionSpaceWrapper import ReduceActionSpaceWrapper\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import os\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from CybORG.Agents.Wrappers.rllib_wrapper import RLlibWrapper\n",
    "import warnings\n",
    "import numpy as np\n",
    "from ray import air, tune\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config: dict):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)\n",
    "    return env\n",
    "\n",
    "def print_results(results_dict):\n",
    "    train_iter = results_dict[\"training_iteration\"]\n",
    "    r_mean = results_dict[\"episode_reward_mean\"]\n",
    "    r_max = results_dict[\"episode_reward_max\"]\n",
    "    r_min = results_dict[\"episode_reward_min\"]\n",
    "    print(f\"{train_iter:4d} \\tr_mean: {r_mean:.1f} \\tr_max: {r_max:.1f} \\tr_min: {r_min: .1f}\")\n",
    "\n",
    "register_env(name=\"CybORG\", env_creator=env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 11:51:01,853\tINFO worker.py:1528 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-12-16 11:54:15</td></tr>\n",
       "<tr><td>Running for: </td><td>00:03:12.11        </td></tr>\n",
       "<tr><td>Memory:      </td><td>36.6/125.8 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 31.0/40 CPUs, 1.0/1 GPUs, 0.0/74.93 GiB heap, 0.0/36.1 GiB objects (0.0/1.0 accelerator_type:V100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_worker\n",
       "s</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CybORG_eaf68_00000</td><td>RUNNING </td><td>172.28.0.2:29337</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         161.754</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">-149.249</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">               -28.8</td><td style=\"text-align: right;\">              -199.8</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=29337)\u001b[0m 2022-12-16 11:51:10,493\tINFO algorithm.py:2303 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPO pid=29337)\u001b[0m 2022-12-16 11:51:10,494\tWARNING ppo.py:351 -- `train_batch_size` (3000) cannot be achieved with your other settings (num_workers=30 num_envs_per_worker=2 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 50.\n",
      "\u001b[2m\u001b[36m(PPO pid=29337)\u001b[0m 2022-12-16 11:51:10,494\tINFO ppo.py:379 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPO pid=29337)\u001b[0m 2022-12-16 11:51:10,496\tINFO algorithm.py:457 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29368)\u001b[0m 2022-12-16 11:51:20,569\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(PPO pid=29337)\u001b[0m 2022-12-16 11:51:27,722\tINFO trainable.py:164 -- Trainable.setup took 17.230 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(PPO pid=29337)\u001b[0m 2022-12-16 11:51:27,722\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                            </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname    </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip   </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_recreated_workers</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                              </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                 </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                                 </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CybORG_eaf68_00000</td><td style=\"text-align: right;\">                  96000</td><td>{&#x27;num_env_steps_sampled&#x27;: 96000, &#x27;num_env_steps_trained&#x27;: 96000, &#x27;num_agent_steps_sampled&#x27;: 96000, &#x27;num_agent_steps_trained&#x27;: 96000}</td><td>{}              </td><td>2022-12-16_11-54-16</td><td>False </td><td style=\"text-align: right;\">               100</td><td>{}             </td><td style=\"text-align: right;\">               -83.8</td><td style=\"text-align: right;\">             -141.936</td><td style=\"text-align: right;\">              -194.8</td><td style=\"text-align: right;\">                  60</td><td style=\"text-align: right;\">             960</td><td>94130e228e3a4e52a09607cd12c99ff0</td><td>01589170c3ff</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.20000000298023224, &#x27;cur_lr&#x27;: 4.999999873689376e-06, &#x27;total_loss&#x27;: 6.194669, &#x27;policy_loss&#x27;: -0.0087248515, &#x27;vf_loss&#x27;: 6.202692, &#x27;vf_explained_var&#x27;: -0.13182797, &#x27;kl&#x27;: 0.0035061329, &#x27;entropy&#x27;: 3.9272957, &#x27;entropy_coeff&#x27;: 0.0, &#x27;model&#x27;: {}}, &#x27;train&#x27;: None}}, &#x27;num_env_steps_sampled&#x27;: 96000, &#x27;num_env_steps_trained&#x27;: 96000, &#x27;num_agent_steps_sampled&#x27;: 96000, &#x27;num_agent_steps_trained&#x27;: 96000}</td><td style=\"text-align: right;\">                        32</td><td>172.28.0.2</td><td style=\"text-align: right;\">                    96000</td><td style=\"text-align: right;\">                    96000</td><td style=\"text-align: right;\">                  96000</td><td style=\"text-align: right;\">                             3000</td><td style=\"text-align: right;\">                  96000</td><td style=\"text-align: right;\">                             3000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                   30</td><td style=\"text-align: right;\">                      0</td><td style=\"text-align: right;\">                         3000</td><td>{&#x27;cpu_util_percent&#x27;: 32.400000000000006, &#x27;ram_util_percent&#x27;: 29.1}</td><td style=\"text-align: right;\">29337</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 3.772123528942345, &#x27;mean_inference_ms&#x27;: 1.52022177450971, &#x27;mean_action_processing_ms&#x27;: 0.19002107004502677, &#x27;mean_env_wait_ms&#x27;: 21.373151031846223, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: -83.79999999999984, &#x27;episode_reward_min&#x27;: -194.79999999999956, &#x27;episode_reward_mean&#x27;: -141.93599999999975, &#x27;episode_len_mean&#x27;: 100.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 60, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [-185.7999999999997, -127.4999999999999, -178.7999999999997, -126.79999999999974, -113.79999999999981, -165.7999999999997, -146.79999999999978, -147.7999999999998, -148.7999999999998, -156.79999999999973, -161.79999999999964, -139.79999999999978, -142.19999999999996, -158.79999999999967, -119.7999999999998, -123.40000000000003, -147.6999999999997, -158.79999999999964, -144.39999999999978, -169.79999999999967, -165.69999999999968, -160.7999999999997, -174.79999999999967, -117.7999999999997, -172.7999999999997, -181.79999999999967, -111.79999999999984, -143.7999999999997, -154.79999999999967, -145.7999999999997, -162.69999999999965, -144.10000000000002, -173.79999999999967, -123.5999999999998, -173.79999999999967, -140.79999999999973, -123.6999999999998, -123.79999999999978, -134.59999999999985, -127.79999999999977, -154.79999999999973, -125.79999999999977, -153.69999999999976, -163.7999999999997, -140.79999999999976, -118.7999999999998, -129.79999999999978, -116.69999999999976, -118.79999999999977, -133.69999999999973, -138.69999999999976, -118.9, -128.79999999999973, -151.69999999999965, -185.6999999999997, -194.79999999999956, -157.79999999999967, -128.79999999999978, -142.6999999999998, -149.79999999999973, -169.7999999999997, -109.79999999999983, -122.69999999999978, -121.79999999999974, -168.79999999999976, -142.79999999999967, -123.79999999999973, -144.79999999999976, -132.79999999999978, -157.7999999999997, -163.7999999999997, -133.79999999999976, -101.39999999999993, -100.79999999999983, -184.79999999999964, -122.79999999999977, -121.19999999999997, -83.79999999999984, -159.79999999999973, -161.19999999999987, -124.79999999999977, -139.79999999999978, -123.79999999999977, -139.6999999999998, -179.7999999999997, -109.79999999999984, -130.7999999999997, -179.69999999999962, -127.50000000000001, -118.6999999999998, -134.79999999999973, -131.7999999999997, -132.79999999999978, -131.7999999999997, -130.79999999999976, -120.70000000000005, -154.79999999999978, -118.69999999999986, -132.69999999999968, -123.79999999999974], &#x27;episode_lengths&#x27;: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 3.772123528942345, &#x27;mean_inference_ms&#x27;: 1.52022177450971, &#x27;mean_action_processing_ms&#x27;: 0.19002107004502677, &#x27;mean_env_wait_ms&#x27;: 21.373151031846223, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0}</td><td style=\"text-align: right;\">             167.238</td><td style=\"text-align: right;\">           5.48474</td><td style=\"text-align: right;\">       167.238</td><td>{&#x27;training_iteration_time_ms&#x27;: 5164.47, &#x27;load_time_ms&#x27;: 2.338, &#x27;load_throughput&#x27;: 1283132.648, &#x27;learn_time_ms&#x27;: 3646.482, &#x27;learn_throughput&#x27;: 822.711, &#x27;synch_weights_time_ms&#x27;: 10.175}</td><td style=\"text-align: right;\"> 1671191656</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">            96000</td><td style=\"text-align: right;\">                  32</td><td>eaf68_00000</td><td style=\"text-align: right;\">       17.246</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tune.Tuner(\n",
    "        \"PPO\",\n",
    "        run_config=air.RunConfig(\n",
    "            stop={\"timesteps_total\": 2e6},\n",
    "            local_dir='results/APPO', name=\"explore2\",\n",
    "            checkpoint_config=air.CheckpointConfig(\n",
    "                checkpoint_frequency=500, \n",
    "            ),\n",
    "        ),\n",
    "        param_space={\n",
    "            # CC3 specific.\n",
    "            \"env\": \"CybORG\",\n",
    "            # General\n",
    "            \"num_gpus\": 1,\n",
    "            \"num_workers\": 30,\n",
    "            \"horizon\": 100,\n",
    "            \"num_envs_per_worker\": 2,\n",
    "            #\"exploration_config\": tune.grid_search([{\"type\": \"RE3\",\n",
    "            #    \"embeds_dim\": 128,\n",
    "            #    \"beta_schedule\": \"constant\",\n",
    "            #    \"sub_exploration\": {\n",
    "            #        \"type\": \"StochasticSampling\",\n",
    "            #},}, \n",
    "            \"exploration_config\": {\"type\": \"StochasticSampling\"},\n",
    "            #algo params\n",
    "            \"train_batch_size\": 3000,\n",
    "            \"lr\": 0.000005,\n",
    "            \"gamma\": 0.9,\n",
    "            \"framework\": 'tf',\n",
    "            \"model\": {\n",
    "                    \"fcnet_hiddens\": [256, 256],\n",
    "                    \"fcnet_activation\": \"relu\",\n",
    "                    #'use_lstm': True,\n",
    "                    #'max_seq_len': 5,\n",
    "                },\n",
    "            \"output\": \"dataset\",\n",
    "            \"output_config\": {\n",
    "                \"format\": \"json\",\n",
    "                \"path\": \"/logs/APPO/StochasticSampling\"},\n",
    "            \"output_compress_columns\": ['prev_actions', 'prev_rewards', 'dones', 't', 'eps_id', 'unroll_id', 'agent_index', 'action_prob', 'action_logp', 'action_dist_inputs', 'advantages', 'value_targets']\n",
    "        },\n",
    "    ).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec  5 12:03:44 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.03   Driver Version: 450.119.03   CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:83:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    38W / 250W |   2167MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(inspect.getfile(CybORG))\n",
    "path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "env = RLlibWrapper(env=cyborg, agent_name=\"Blue\", max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0030630571070921465"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = np.array([0.9997, 0.0001, 0.0001, 0.0001])\n",
    "entropy = -np.sum(dist * np.log(dist))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6949892486343404"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = np.array([0.5, 0.5, 0.0001, 0.0001])\n",
    "entropy = -np.sum(dist * np.log(dist))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "entropy = -np.sum(dist * np.log(dist))\n",
    "entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
